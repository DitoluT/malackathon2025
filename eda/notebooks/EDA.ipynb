{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f341e157",
   "metadata": {},
   "source": [
    "# üß† Data Cleaning y An√°lisis Exploratorio - CMBD Salud Mental\n",
    "## Competici√≥n Malackaton 2025\n",
    "\n",
    "> **Pipeline completo: Limpieza de datos + EDA del Conjunto M√≠nimo B√°sico de Datos (CMBD)**  \n",
    "> **Enfermedades Mentales - An√°lisis seg√∫n est√°ndares del Ministerio de Sanidad**\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ **OBJETIVOS ESPEC√çFICOS DE LA COMPETICI√ìN**\n",
    "\n",
    "### ‚úÖ **Objetivo 1: An√°lisis Descriptivo Inicial**\n",
    "- Estudio estad√≠stico elemental de las variables\n",
    "- Identificaci√≥n de tipos de datos (fecha, car√°cter, categ√≥ricos, num√©ricos, etc.)\n",
    "- Detecci√≥n de valores nulos o desconocidos\n",
    "- Identificaci√≥n de outliers y anomal√≠as\n",
    "\n",
    "### ‚úÖ **Objetivo 2: Ingenier√≠a de Caracter√≠sticas**\n",
    "- Nuevas variables √∫tiles en siguientes fases\n",
    "- Creaci√≥n, transformaci√≥n y codificaci√≥n de variables\n",
    "- Preparaci√≥n para an√°lisis predictivos\n",
    "\n",
    "---\n",
    "\n",
    "## üìã **METODOLOG√çA PROFESIONAL**\n",
    "\n",
    "Este proyecto sigue las **mejores pr√°cticas de Data Science** con separaci√≥n clara entre:\n",
    "\n",
    "1. **üßπ DATA CLEANING** - Preparaci√≥n y limpieza de datos\n",
    "2. **üîç EXPLORATORY DATA ANALYSIS (EDA)** - An√°lisis exploratorio\n",
    "\n",
    "---\n",
    "\n",
    "### üóÇÔ∏è **√çNDICE DE CONTENIDOS**\n",
    "\n",
    "#### üßπ **FASE 1: DATA CLEANING**\n",
    "1. [‚öôÔ∏è Configuraci√≥n del Entorno](#config)\n",
    "2. [üì• Carga y Validaci√≥n Inicial](#carga)\n",
    "3. [üßπ Limpieza de Datos CMBD](#cleaning)\n",
    "4. [‚úÖ Validaci√≥n Post-Limpieza](#validacion)\n",
    "\n",
    "#### üîç **FASE 2: EXPLORATORY DATA ANALYSIS (EDA)**\n",
    "5. [üìä An√°lisis Descriptivo Inicial](#descriptivo)\n",
    "6. [üî¢ An√°lisis de Variables Num√©ricas](#numericas)  \n",
    "7. [üè∑Ô∏è An√°lisis de Variables Categ√≥ricas](#categoricas)\n",
    "8. [üîó An√°lisis Bivariado y Correlaciones](#bivariado)\n",
    "9. [üõ†Ô∏è Ingenier√≠a de Caracter√≠sticas](#ingenieria)\n",
    "10. [üìà Insights y Hallazgos Clave](#insights)\n",
    "11. [üìã Resumen Ejecutivo](#resumen)\n",
    "\n",
    "---\n",
    "\n",
    "> **üìñ Referencia**: An√°lisis basado en \"Anexo solicitud RAE CMBD 2018\" - Ministerio de Sanidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed2cb3a",
   "metadata": {},
   "source": [
    "# üßπ FASE 1: DATA CLEANING\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è 1. Configuraci√≥n del Entorno {#config}\n",
    "\n",
    "### Importaci√≥n de Librer√≠as y Configuraci√≥n Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3f2516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURACI√ìN INICIAL - DATA CLEANING & EDA\n",
    "# ============================================================================\n",
    "\n",
    "# Librer√≠as esenciales para Data Science\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuraci√≥n inicial\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"üßπ CONFIGURACI√ìN PARA DATA CLEANING\")\n",
    "print(\"=\"*50)\n",
    "print(f\"‚úÖ pandas: {pd.__version__}\")\n",
    "print(f\"‚úÖ numpy: {np.__version__}\")\n",
    "print(\"‚úÖ Entorno configurado para limpieza de datos CMBD\")\n",
    "print(\"üìñ Referencia: Anexo solicitud RAE CMBD 2018\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d9b674",
   "metadata": {},
   "source": [
    "### Funciones de Limpieza Est√°ndar para CMBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2b0f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FUNCIONES DE LIMPIEZA EST√ÅNDAR PARA CMBD\n",
    "# ============================================================================\n",
    "\n",
    "# Definir los mappings para limpieza\n",
    "REPLACEMENT_MAPPING = {\n",
    "    '√°': 'a', '√©': 'e', '√≠': 'i', '√≥': 'o', '√∫': 'u',\n",
    "    '√±': 'n', '√º': 'u', '√ß': 'c', \n",
    "    '√Å': 'A', '√â': 'E', '√ç': 'I', '√ì': 'O', '√ö': 'U',\n",
    "    '√ë': 'N', '√ú': 'U', '√á': 'C',\n",
    "    ',': '_', '-': '_', '/': '_', ' ': '_', '.': '_',\n",
    "    '(': '', ')': '', '[': '', ']': '', '{': '', '}': '',\n",
    "    '¬∫': '', '¬™': ''\n",
    "}\n",
    "\n",
    "ARTICLES_MAPPING = {article: '_' for article in [\n",
    "    '_el_', '_la_', '_los_', '_las_', '_un_', '_una_', '_unos_', '_unas_',\n",
    "    '_del_', '_de_', '_y_', '_o_'\n",
    "]}\n",
    "\n",
    "def clean_column_names(df):\n",
    "    \"\"\"\n",
    "    Normaliza los nombres de las columnas de un DataFrame siguiendo el est√°ndar snake_case.\n",
    "    - Convierte a min√∫sculas\n",
    "    - Reemplaza tildes y caracteres especiales\n",
    "    - Elimina art√≠culos comunes\n",
    "    - Reemplaza espacios y caracteres especiales por guiones bajos\n",
    "    - Elimina guiones bajos m√∫ltiples consecutivos\n",
    "    \"\"\"\n",
    "    new_columns = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        # Convertir a string por si acaso\n",
    "        col_str = str(col)\n",
    "        \n",
    "        # Reemplazar caracteres especiales y tildes\n",
    "        for old, new in REPLACEMENT_MAPPING.items():\n",
    "            col_str = col_str.replace(old, new)\n",
    "        \n",
    "        # Convertir a min√∫sculas\n",
    "        col_str = col_str.lower()\n",
    "        \n",
    "        # A√±adir guiones bajos al inicio y final para facilitar la eliminaci√≥n de art√≠culos\n",
    "        col_str = '_' + col_str + '_'\n",
    "        \n",
    "        # Eliminar art√≠culos\n",
    "        for article, replacement in ARTICLES_MAPPING.items():\n",
    "            col_str = col_str.replace(article, replacement)\n",
    "        \n",
    "        # Eliminar guiones bajos m√∫ltiples consecutivos\n",
    "        while '__' in col_str:\n",
    "            col_str = col_str.replace('__', '_')\n",
    "        \n",
    "        # Eliminar guiones bajos al inicio y al final\n",
    "        col_str = col_str.strip('_')\n",
    "        \n",
    "        new_columns.append(col_str)\n",
    "    \n",
    "    # Asignar los nuevos nombres\n",
    "    df.columns = new_columns\n",
    "    \n",
    "    return df\n",
    "\n",
    "def clean_string_series(series):\n",
    "    \"\"\"\n",
    "    Limpia una Serie de pandas normalizando texto:\n",
    "    - Convierte a min√∫sculas\n",
    "    - Reemplaza tildes y caracteres especiales\n",
    "    - Elimina art√≠culos comunes\n",
    "    - Elimina espacios extras\n",
    "    \"\"\"\n",
    "    # Convertir a min√∫sculas\n",
    "    series = series.str.lower()\n",
    "    \n",
    "    # Reemplazar tildes y caracteres especiales\n",
    "    for old, new in REPLACEMENT_MAPPING.items():\n",
    "        series = series.str.replace(old, new, regex=False)\n",
    "    \n",
    "    # A√±adir espacios alrededor para eliminar art√≠culos\n",
    "    series = ' ' + series + ' '\n",
    "    \n",
    "    # Eliminar art√≠culos (convertir mapping de _ a espacio)\n",
    "    articles_space = {k.replace('_', ' '): ' ' for k in ARTICLES_MAPPING.keys()}\n",
    "    for article, replacement in articles_space.items():\n",
    "        series = series.str.replace(article, replacement, regex=False)\n",
    "    \n",
    "    # Eliminar espacios m√∫ltiples\n",
    "    series = series.str.replace(r'\\s+', ' ', regex=True)\n",
    "    \n",
    "    # Eliminar espacios al inicio y final\n",
    "    series = series.str.strip()\n",
    "    \n",
    "    return series\n",
    "\n",
    "print(\"‚úÖ Funciones de limpieza definidas correctamente\")\n",
    "print(\"üßπ Listas para aplicar est√°ndares de normalizaci√≥n CMBD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caac17fa",
   "metadata": {},
   "source": [
    "## üì• 2. Carga y Validaci√≥n Inicial {#carga}\n",
    "\n",
    "### Carga del Dataset CMBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1c9572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DEFINICIONES CMBD - MINISTERIO DE SANIDAD (2018)\n",
    "# ============================================================================\n",
    "\n",
    "# Definici√≥n de dominios seg√∫n Anexo solicitud RAE CMBD 2018\n",
    "CMBD_DOMAINS = {\n",
    "    'SEXO': {\n",
    "        1: 'Var√≥n',\n",
    "        2: 'Mujer', \n",
    "        3: 'Indeterminado',\n",
    "        9: 'No especificado'\n",
    "    },\n",
    "    'TIPO_INGRESO': {\n",
    "        1: 'Urgente',\n",
    "        2: 'Programado',\n",
    "        9: 'No especificado'\n",
    "    },\n",
    "    'TIPO_ALTA': {\n",
    "        1: 'Domicilio',\n",
    "        2: 'Traslado a otro hospital',\n",
    "        3: 'Alta voluntaria',\n",
    "        4: '√âxitus',\n",
    "        5: 'Traslado a centro sociosanitario',\n",
    "        9: 'Otros'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Configuraci√≥n de rutas y carga de datos\n",
    "def load_cmbd_dataset():\n",
    "    \"\"\"\n",
    "    Carga el dataset CMBD con validaci√≥n de estructura\n",
    "    \"\"\"\n",
    "    print(\"üì• CARGA DEL DATASET CMBD\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Rutas posibles para el archivo\n",
    "    possible_paths = [\n",
    "        'SaludMental.xls - enfermedadesMentalesDiagnostico.csv',\n",
    "        '../raw_data/SaludMental.xls - enfermedadesMentalesDiagnostico.csv',\n",
    "        'SaludMental.csv',\n",
    "        '../raw_data/SaludMental.csv',\n",
    "        'data.csv'\n",
    "    ]\n",
    "    \n",
    "    for file_path in possible_paths:\n",
    "        if os.path.exists(file_path):\n",
    "            try:\n",
    "                # Intentar diferentes codificaciones comunes en Espa√±a\n",
    "                encodings = ['utf-8', 'latin1', 'iso-8859-1', 'cp1252', 'windows-1252']\n",
    "                \n",
    "                for encoding in encodings:\n",
    "                    try:\n",
    "                        print(f\"üîÑ Cargando {file_path} con codificaci√≥n {encoding}...\")\n",
    "                        df = pd.read_csv(file_path, encoding=encoding)\n",
    "                        print(f\"‚úÖ Archivo cargado exitosamente\")\n",
    "                        print(f\"üìä Dimensiones: {df.shape[0]:,} filas √ó {df.shape[1]} columnas\")\n",
    "                        return df, file_path\n",
    "                    except UnicodeDecodeError:\n",
    "                        continue\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error con {file_path}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    # Dataset de ejemplo con estructura CMBD\n",
    "    print(\"‚ö†Ô∏è Archivo no encontrado. Creando dataset CMBD de ejemplo...\")\n",
    "    return create_cmbd_sample(), \"dataset_ejemplo_cmbd\"\n",
    "\n",
    "def create_cmbd_sample():\n",
    "    \"\"\"Crear dataset de ejemplo con estructura CMBD real\"\"\"\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1500\n",
    "    \n",
    "    # Generar datos siguiendo distribuciones realistas de salud mental\n",
    "    data = {\n",
    "        'Comunidad Aut√≥noma': np.random.choice([\n",
    "            'Andaluc√≠a', 'Arag√≥n', 'Asturias', 'Baleares', 'Canarias',\n",
    "            'Cantabria', 'Castilla y Le√≥n', 'Castilla-La Mancha', 'Catalu√±a',\n",
    "            'Valencia', 'Extremadura', 'Galicia', 'Madrid', 'Murcia',\n",
    "            'Navarra', 'Pa√≠s Vasco', 'Rioja', 'Ceuta', 'Melilla'\n",
    "        ], n_samples, p=[0.18, 0.03, 0.02, 0.025, 0.045, 0.01, 0.05, 0.04, 0.16, \n",
    "                        0.11, 0.02, 0.055, 0.14, 0.03, 0.015, 0.046, 0.007, 0.003, 0.002]),\n",
    "        \n",
    "        'Sexo': np.random.choice([1, 2, 3, 9], n_samples, p=[0.45, 0.52, 0.02, 0.01]),\n",
    "        \n",
    "        'Edad': np.clip(np.random.gamma(2, 20), 0, 100).astype(int),\n",
    "        \n",
    "        'Estancia D√≠as': np.clip(np.random.exponential(8) + 1, 1, 365).astype(int),\n",
    "        \n",
    "        'Coste APR': np.random.lognormal(8.5, 0.8),\n",
    "        \n",
    "        'Categor√≠a': np.random.choice([\n",
    "            'Trastornos del estado de √°nimo [afectivos]',\n",
    "            'Esquizofrenia, trastornos esquizot√≠picos y trastornos de ideas delirantes', \n",
    "            'Trastornos neur√≥ticos, secundarios a situaciones estresantes y somatomorfos',\n",
    "            'Trastornos del comportamiento y de las emociones',\n",
    "            'Trastornos mentales org√°nicos, incluidos los sintom√°ticos',\n",
    "            'Trastornos de la personalidad y del comportamiento del adulto'\n",
    "        ], n_samples, p=[0.35, 0.20, 0.18, 0.12, 0.10, 0.05]),\n",
    "        \n",
    "        'Fecha Ingreso': pd.date_range('2020-01-01', '2023-12-31', n_samples),\n",
    "        \n",
    "        'Tipo Ingreso': np.random.choice([1, 2, 9], n_samples, p=[0.75, 0.23, 0.02]),\n",
    "        \n",
    "        'Tipo Alta': np.random.choice([1, 2, 3, 4, 5, 9], n_samples, p=[0.80, 0.08, 0.05, 0.02, 0.03, 0.02])\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Cargar el dataset\n",
    "df_raw, source_file = load_cmbd_dataset()\n",
    "\n",
    "print(f\"\\nüìÅ Fuente: {source_file}\")\n",
    "print(f\"üìã Dataset cargado para limpieza y an√°lisis\")\n",
    "print(f\"üè• Est√°ndar CMBD - Ministerio de Sanidad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34852234",
   "metadata": {},
   "source": [
    "## üßπ 3. Limpieza de Datos CMBD {#cleaning}\n",
    "\n",
    "### Aplicaci√≥n de Est√°ndares de Normalizaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51041921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PROCESO DE LIMPIEZA DE DATOS CMBD\n",
    "# ============================================================================\n",
    "\n",
    "def comprehensive_data_cleaning(df):\n",
    "    \"\"\"\n",
    "    Proceso completo de limpieza de datos CMBD\n",
    "    \"\"\"\n",
    "    print(\"üßπ INICIANDO PROCESO DE LIMPIEZA DE DATOS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Crear copia para mantener datos originales\n",
    "    df_clean = df.copy()\n",
    "    cleaning_report = {'steps': [], 'changes': []}\n",
    "    \n",
    "    # PASO 1: Limpieza de nombres de columnas\n",
    "    print(\"\\nüìã PASO 1: Normalizaci√≥n de nombres de columnas\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    original_columns = list(df_clean.columns)\n",
    "    df_clean = clean_column_names(df_clean)\n",
    "    new_columns = list(df_clean.columns)\n",
    "    \n",
    "    # Mostrar cambios en nombres de columnas\n",
    "    for old, new in zip(original_columns, new_columns):\n",
    "        if old != new:\n",
    "            print(f\"   ‚Ä¢ '{old}' ‚Üí '{new}'\")\n",
    "            cleaning_report['changes'].append(f\"Columna renombrada: {old} ‚Üí {new}\")\n",
    "    \n",
    "    cleaning_report['steps'].append(\"Normalizaci√≥n de nombres de columnas\")\n",
    "    \n",
    "    # PASO 2: Limpieza de variables categ√≥ricas de texto\n",
    "    print(\"\\nüî§ PASO 2: Limpieza de variables categ√≥ricas\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    categorical_columns = df_clean.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        if df_clean[col].dtype == 'object':\n",
    "            print(f\"   üßπ Limpiando columna: {col}\")\n",
    "            \n",
    "            # Guardar valores originales √∫nicos (muestra)\n",
    "            original_unique = df_clean[col].dropna().unique()[:5]\n",
    "            \n",
    "            # Aplicar limpieza\n",
    "            df_clean[col] = clean_string_series(df_clean[col])\n",
    "            \n",
    "            # Mostrar cambios (muestra)\n",
    "            new_unique = df_clean[col].dropna().unique()[:5]\n",
    "            if len(original_unique) > 0 and len(new_unique) > 0:\n",
    "                print(f\"     Ejemplo: '{original_unique[0]}' ‚Üí '{new_unique[0]}'\")\n",
    "            \n",
    "            cleaning_report['changes'].append(f\"Limpieza texto aplicada a: {col}\")\n",
    "    \n",
    "    cleaning_report['steps'].append(\"Limpieza de variables categ√≥ricas\")\n",
    "    \n",
    "    # PASO 3: Estandarizaci√≥n de variable SEXO seg√∫n CMBD\n",
    "    print(\"\\nüöª PASO 3: Estandarizaci√≥n variable SEXO\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    sexo_columns = [col for col in df_clean.columns if 'sexo' in col.lower()]\n",
    "    \n",
    "    if sexo_columns:\n",
    "        sexo_col = sexo_columns[0]\n",
    "        print(f\"   üìä Procesando columna: {sexo_col}\")\n",
    "        \n",
    "        # Mostrar distribuci√≥n original\n",
    "        original_dist = df_clean[sexo_col].value_counts()\n",
    "        print(f\"   üìà Distribuci√≥n original:\")\n",
    "        for val, count in original_dist.items():\n",
    "            print(f\"      {val}: {count}\")\n",
    "        \n",
    "        # Mapear seg√∫n est√°ndar CMBD\n",
    "        def map_sexo_cmbd(value):\n",
    "            if pd.isna(value):\n",
    "                return 9  # No especificado\n",
    "            elif value in [1, 1.0, '1']:\n",
    "                return 1  # Var√≥n\n",
    "            elif value in [2, 2.0, '2']:\n",
    "                return 2  # Mujer\n",
    "            elif value in [3, 3.0, '3']:\n",
    "                return 3  # Indeterminado\n",
    "            else:\n",
    "                return 9  # No especificado por defecto\n",
    "        \n",
    "        df_clean[sexo_col] = df_clean[sexo_col].apply(map_sexo_cmbd)\n",
    "        \n",
    "        # Crear columna con etiquetas descriptivas\n",
    "        df_clean[f'{sexo_col}_etiqueta'] = df_clean[sexo_col].map(CMBD_DOMAINS['SEXO'])\n",
    "        \n",
    "        # Mostrar distribuci√≥n final\n",
    "        final_dist = df_clean[sexo_col].value_counts()\n",
    "        print(f\"   ‚úÖ Distribuci√≥n estandarizada:\")\n",
    "        for val, count in final_dist.items():\n",
    "            label = CMBD_DOMAINS['SEXO'].get(val, 'Desconocido')\n",
    "            print(f\"      {val} ({label}): {count}\")\n",
    "        \n",
    "        cleaning_report['changes'].append(f\"Variable SEXO estandarizada seg√∫n CMBD\")\n",
    "    \n",
    "    cleaning_report['steps'].append(\"Estandarizaci√≥n variable SEXO\")\n",
    "    \n",
    "    # PASO 4: Limpieza de variables num√©ricas\n",
    "    print(\"\\nüî¢ PASO 4: Validaci√≥n de variables num√©ricas\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    numeric_columns = df_clean.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    for col in numeric_columns:\n",
    "        # Detectar y reportar valores negativos inv√°lidos\n",
    "        if col.lower() in ['edad', 'estancia', 'dias', 'coste', 'costo']:\n",
    "            negative_count = (df_clean[col] < 0).sum()\n",
    "            if negative_count > 0:\n",
    "                print(f\"   ‚ö†Ô∏è {col}: {negative_count} valores negativos detectados\")\n",
    "                # Convertir valores negativos a NaN para revisi√≥n manual\n",
    "                df_clean.loc[df_clean[col] < 0, col] = np.nan\n",
    "                cleaning_report['changes'].append(f\"{col}: {negative_count} valores negativos convertidos a NaN\")\n",
    "        \n",
    "        # Detectar valores extremos\n",
    "        if col.lower() == 'edad':\n",
    "            invalid_age = ((df_clean[col] > 120) | (df_clean[col] < 0)).sum()\n",
    "            if invalid_age > 0:\n",
    "                print(f\"   ‚ö†Ô∏è {col}: {invalid_age} valores de edad inv√°lidos (>120 o <0)\")\n",
    "                cleaning_report['changes'].append(f\"{col}: {invalid_age} edades inv√°lidas detectadas\")\n",
    "    \n",
    "    cleaning_report['steps'].append(\"Validaci√≥n de variables num√©ricas\")\n",
    "    \n",
    "    # PASO 5: Procesamiento de fechas\n",
    "    print(\"\\nüìÖ PASO 5: Procesamiento de variables fecha\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    date_columns = [col for col in df_clean.columns if 'fecha' in col.lower() or 'ingreso' in col.lower()]\n",
    "    \n",
    "    for col in date_columns:\n",
    "        if df_clean[col].dtype == 'object':\n",
    "            try:\n",
    "                print(f\"   üìÖ Procesando: {col}\")\n",
    "                df_clean[col] = pd.to_datetime(df_clean[col], errors='coerce')\n",
    "                \n",
    "                # Validar rangos de fechas razonables (√∫ltimo siglo)\n",
    "                min_date = pd.Timestamp('1920-01-01')\n",
    "                max_date = pd.Timestamp.now()\n",
    "                \n",
    "                invalid_dates = ((df_clean[col] < min_date) | (df_clean[col] > max_date)).sum()\n",
    "                if invalid_dates > 0:\n",
    "                    print(f\"   ‚ö†Ô∏è {invalid_dates} fechas fuera del rango v√°lido\")\n",
    "                \n",
    "                cleaning_report['changes'].append(f\"{col}: convertida a datetime\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Error procesando {col}: {e}\")\n",
    "    \n",
    "    cleaning_report['steps'].append(\"Procesamiento de fechas\")\n",
    "    \n",
    "    # RESUMEN DEL PROCESO\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä RESUMEN DEL PROCESO DE LIMPIEZA\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"‚úÖ Pasos completados: {len(cleaning_report['steps'])}\")\n",
    "    for i, step in enumerate(cleaning_report['steps'], 1):\n",
    "        print(f\"   {i}. {step}\")\n",
    "    \n",
    "    print(f\"\\nüîÑ Cambios aplicados: {len(cleaning_report['changes'])}\")\n",
    "    for change in cleaning_report['changes']:\n",
    "        print(f\"   ‚Ä¢ {change}\")\n",
    "    \n",
    "    print(f\"\\nüìè Dimensiones finales: {df_clean.shape[0]:,} filas √ó {df_clean.shape[1]} columnas\")\n",
    "    \n",
    "    return df_clean, cleaning_report\n",
    "\n",
    "# Aplicar proceso de limpieza completo\n",
    "df_clean, cleaning_log = comprehensive_data_cleaning(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a48394a",
   "metadata": {},
   "source": [
    "## ‚úÖ 4. Validaci√≥n Post-Limpieza {#validacion}\n",
    "\n",
    "### Verificaci√≥n de Calidad de Datos Limpios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478e7689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VALIDACI√ìN POST-LIMPIEZA\n",
    "# ============================================================================\n",
    "\n",
    "def validate_clean_data(df_clean, cleaning_log):\n",
    "    \"\"\"\n",
    "    Validaci√≥n exhaustiva de datos despu√©s de limpieza\n",
    "    \"\"\"\n",
    "    print(\"‚úÖ VALIDACI√ìN DE DATOS LIMPIOS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    validation_report = {}\n",
    "    \n",
    "    # 1. Verificaci√≥n b√°sica de estructura\n",
    "    print(\"\\nüìä VERIFICACI√ìN B√ÅSICA:\")\n",
    "    print(f\"   ‚Ä¢ Filas: {df_clean.shape[0]:,}\")\n",
    "    print(f\"   ‚Ä¢ Columnas: {df_clean.shape[1]:,}\")\n",
    "    print(f\"   ‚Ä¢ Memoria utilizada: {df_clean.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    validation_report['shape'] = df_clean.shape\n",
    "    validation_report['memory_mb'] = df_clean.memory_usage(deep=True).sum() / 1024**2\n",
    "    \n",
    "    # 2. An√°lisis de tipos de datos\n",
    "    print(\"\\nüî¢ TIPOS DE DATOS:\")\n",
    "    dtype_counts = df_clean.dtypes.value_counts()\n",
    "    for dtype, count in dtype_counts.items():\n",
    "        print(f\"   ‚Ä¢ {dtype}: {count} columnas\")\n",
    "        validation_report[f'dtype_{dtype}'] = count\n",
    "    \n",
    "    # 3. An√°lisis de completitud\n",
    "    print(\"\\nüíØ AN√ÅLISIS DE COMPLETITUD:\")\n",
    "    missing_data = df_clean.isnull().sum()\n",
    "    total_cells = df_clean.shape[0] * df_clean.shape[1]\n",
    "    total_missing = missing_data.sum()\n",
    "    completeness_pct = ((total_cells - total_missing) / total_cells) * 100\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Completitud general: {completeness_pct:.2f}%\")\n",
    "    \n",
    "    if total_missing > 0:\n",
    "        print(f\"   ‚Ä¢ Valores faltantes: {total_missing:,} ({total_missing/total_cells*100:.2f}%)\")\n",
    "        print(f\"   ‚Ä¢ Variables con valores faltantes:\")\n",
    "        \n",
    "        missing_vars = missing_data[missing_data > 0].sort_values(ascending=False)\n",
    "        for var, count in missing_vars.head(10).items():\n",
    "            pct = count / df_clean.shape[0] * 100\n",
    "            print(f\"     - {var}: {count:,} ({pct:.1f}%)\")\n",
    "    else:\n",
    "        print(\"   ‚Ä¢ ‚úÖ No hay valores faltantes\")\n",
    "    \n",
    "    validation_report['completeness_pct'] = completeness_pct\n",
    "    validation_report['missing_values'] = total_missing\n",
    "    \n",
    "    # 4. Verificaci√≥n de duplicados\n",
    "    print(\"\\nüîÑ VERIFICACI√ìN DE DUPLICADOS:\")\n",
    "    duplicates = df_clean.duplicated().sum()\n",
    "    duplicates_pct = duplicates / len(df_clean) * 100\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Registros duplicados: {duplicates:,} ({duplicates_pct:.2f}%)\")\n",
    "    \n",
    "    if duplicates > 0:\n",
    "        print(\"   ‚Ä¢ ‚ö†Ô∏è Se recomienda revisar duplicados\")\n",
    "    else:\n",
    "        print(\"   ‚Ä¢ ‚úÖ No hay registros duplicados\")\n",
    "    \n",
    "    validation_report['duplicates'] = duplicates\n",
    "    validation_report['duplicates_pct'] = duplicates_pct\n",
    "    \n",
    "    # 5. Validaci√≥n espec√≠fica CMBD\n",
    "    print(\"\\nüè• VALIDACI√ìN ESPEC√çFICA CMBD:\")\n",
    "    \n",
    "    # Verificar variable SEXO\n",
    "    sexo_cols = [col for col in df_clean.columns if 'sexo' in col.lower()]\n",
    "    if sexo_cols:\n",
    "        sexo_col = sexo_cols[0]\n",
    "        sexo_values = df_clean[sexo_col].dropna().unique()\n",
    "        valid_sexo = all(val in [1, 2, 3, 9] for val in sexo_values)\n",
    "        \n",
    "        print(f\"   ‚Ä¢ Variable SEXO ({sexo_col}): {'‚úÖ V√°lida' if valid_sexo else '‚ö†Ô∏è Requiere revisi√≥n'}\")\n",
    "        print(f\"     Valores: {sorted(sexo_values)}\")\n",
    "        \n",
    "        validation_report['sexo_valid'] = valid_sexo\n",
    "    \n",
    "    # Verificar rangos de edad\n",
    "    edad_cols = [col for col in df_clean.columns if 'edad' in col.lower()]\n",
    "    if edad_cols:\n",
    "        edad_col = edad_cols[0]\n",
    "        edad_min = df_clean[edad_col].min()\n",
    "        edad_max = df_clean[edad_col].max()\n",
    "        edad_valid = (edad_min >= 0) and (edad_max <= 120)\n",
    "        \n",
    "        print(f\"   ‚Ä¢ Variable EDAD ({edad_col}): {'‚úÖ V√°lida' if edad_valid else '‚ö†Ô∏è Requiere revisi√≥n'}\")\n",
    "        print(f\"     Rango: {edad_min:.0f} - {edad_max:.0f} a√±os\")\n",
    "        \n",
    "        validation_report['edad_valid'] = edad_valid\n",
    "    \n",
    "    # 6. Resumen de calidad\n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(\"üìã RESUMEN DE CALIDAD DE DATOS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Calcular score de calidad\n",
    "    quality_score = 0\n",
    "    max_score = 100\n",
    "    \n",
    "    # Completitud (40 puntos)\n",
    "    quality_score += (completeness_pct / 100) * 40\n",
    "    \n",
    "    # Sin duplicados (20 puntos)\n",
    "    if duplicates == 0:\n",
    "        quality_score += 20\n",
    "    elif duplicates_pct < 5:\n",
    "        quality_score += 15\n",
    "    elif duplicates_pct < 10:\n",
    "        quality_score += 10\n",
    "    \n",
    "    # Validaci√≥n CMBD (40 puntos)\n",
    "    cmbd_score = 0\n",
    "    if 'sexo_valid' in validation_report and validation_report['sexo_valid']:\n",
    "        cmbd_score += 20\n",
    "    if 'edad_valid' in validation_report and validation_report['edad_valid']:\n",
    "        cmbd_score += 20\n",
    "    \n",
    "    quality_score += cmbd_score\n",
    "    \n",
    "    # Determinar nivel de calidad\n",
    "    if quality_score >= 90:\n",
    "        quality_level = \"üü¢ EXCELENTE\"\n",
    "    elif quality_score >= 80:\n",
    "        quality_level = \"üü° BUENA\"\n",
    "    elif quality_score >= 70:\n",
    "        quality_level = \"üü† ACEPTABLE\"\n",
    "    else:\n",
    "        quality_level = \"üî¥ REQUIERE MEJORA\"\n",
    "    \n",
    "    print(f\"üìä PUNTUACI√ìN DE CALIDAD: {quality_score:.1f}/100 {quality_level}\")\n",
    "    print(f\"   ‚Ä¢ Completitud: {completeness_pct:.1f}% (40 pts)\")\n",
    "    print(f\"   ‚Ä¢ Duplicados: {duplicates_pct:.1f}% ({20 - (duplicates_pct/5)*5:.0f} pts)\")\n",
    "    print(f\"   ‚Ä¢ Validaci√≥n CMBD: {cmbd_score}/40 pts\")\n",
    "    \n",
    "    validation_report['quality_score'] = quality_score\n",
    "    validation_report['quality_level'] = quality_level.split()[1]\n",
    "    \n",
    "    # 7. Recomendaciones\n",
    "    print(f\"\\nüéØ RECOMENDACIONES:\")\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    if completeness_pct < 95:\n",
    "        recommendations.append(f\"Investigar causa de valores faltantes ({100-completeness_pct:.1f}%)\")\n",
    "    \n",
    "    if duplicates > 0:\n",
    "        recommendations.append(f\"Revisar y eliminar {duplicates} registros duplicados\")\n",
    "    \n",
    "    if quality_score < 80:\n",
    "        recommendations.append(\"Realizar limpieza adicional antes del an√°lisis\")\n",
    "    \n",
    "    if not recommendations:\n",
    "        recommendations.append(\"‚úÖ Dataset listo para an√°lisis exploratorio\")\n",
    "    \n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        print(f\"   {i}. {rec}\")\n",
    "    \n",
    "    validation_report['recommendations'] = recommendations\n",
    "    \n",
    "    print(f\"\\n‚úÖ Validaci√≥n completada - Dataset preparado para EDA\")\n",
    "    \n",
    "    return validation_report\n",
    "\n",
    "# Ejecutar validaci√≥n completa\n",
    "validation_results = validate_clean_data(df_clean, cleaning_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fb1dbc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üîç FASE 2: EXPLORATORY DATA ANALYSIS (EDA)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Configuraci√≥n para An√°lisis Exploratorio\n",
    "\n",
    "### Librer√≠as de Visualizaci√≥n y An√°lisis Estad√≠stico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cefcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURACI√ìN PARA EDA - AN√ÅLISIS EXPLORATORIO DE DATOS\n",
    "# ============================================================================\n",
    "\n",
    "# Librer√≠as para visualizaci√≥n avanzada\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# An√°lisis estad√≠stico avanzado\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency, normaltest, jarque_bera, shapiro\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Configuraci√≥n de estilo para visualizaciones profesionales\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (12, 8),\n",
    "    'font.size': 11,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.labelsize': 12,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 10,\n",
    "    'figure.titlesize': 16,\n",
    "    'axes.grid': True,\n",
    "    'grid.alpha': 0.3\n",
    "})\n",
    "\n",
    "# Configuraci√≥n adicional para pandas en EDA\n",
    "pd.set_option('display.precision', 3)\n",
    "\n",
    "print(\"üîç CONFIGURACI√ìN EDA COMPLETADA\")\n",
    "print(\"=\"*50)\n",
    "print(\"‚úÖ Librer√≠as de visualizaci√≥n cargadas\")\n",
    "print(\"‚úÖ Herramientas estad√≠sticas preparadas\") \n",
    "print(\"‚úÖ Configuraci√≥n de gr√°ficos optimizada\")\n",
    "print(\"üìä Listo para an√°lisis exploratorio avanzado\")\n",
    "\n",
    "# Verificar que tenemos datos limpios\n",
    "print(f\"\\nüìã Dataset para EDA:\")\n",
    "print(f\"   ‚Ä¢ Dimensiones: {df_clean.shape[0]:,} filas √ó {df_clean.shape[1]} columnas\")\n",
    "print(f\"   ‚Ä¢ Calidad: {validation_results.get('quality_score', 0):.1f}/100\")\n",
    "print(f\"   ‚Ä¢ Estado: Datos limpios y validados ‚úÖ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637ebe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "### üîç Inspecci√≥n Inicial Detallada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f5048b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para an√°lisis completo de dataset\n",
    "def comprehensive_data_overview(df):\n",
    "    \"\"\"\n",
    "    An√°lisis completo y profesional del dataset\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"üìä RESUMEN EJECUTIVO DEL DATASET\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Informaci√≥n b√°sica\n",
    "    print(f\"\\nüìà INFORMACI√ìN GENERAL:\")\n",
    "    print(f\"   ‚Ä¢ N√∫mero de registros: {df.shape[0]:,}\")\n",
    "    print(f\"   ‚Ä¢ N√∫mero de variables: {df.shape[1]:,}\")\n",
    "    print(f\"   ‚Ä¢ Tama√±o en memoria: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # An√°lisis de tipos de datos\n",
    "    print(f\"\\nüî¢ TIPOS DE DATOS:\")\n",
    "    dtype_counts = df.dtypes.value_counts()\n",
    "    for dtype, count in dtype_counts.items():\n",
    "        print(f\"   ‚Ä¢ {dtype}: {count} variables ({count/df.shape[1]*100:.1f}%)\")\n",
    "    \n",
    "    # An√°lisis de valores nulos\n",
    "    missing_data = df.isnull().sum()\n",
    "    missing_pct = (missing_data / len(df) * 100).round(2)\n",
    "    \n",
    "    print(f\"\\n‚ùå VALORES FALTANTES:\")\n",
    "    if missing_data.sum() == 0:\n",
    "        print(\"   ‚Ä¢ ‚úÖ No hay valores faltantes\")\n",
    "    else:\n",
    "        print(f\"   ‚Ä¢ Total de valores faltantes: {missing_data.sum():,}\")\n",
    "        print(f\"   ‚Ä¢ Porcentaje total: {missing_data.sum()/(df.shape[0]*df.shape[1])*100:.2f}%\")\n",
    "        \n",
    "        # Mostrar columnas con valores faltantes\n",
    "        missing_cols = missing_data[missing_data > 0].sort_values(ascending=False)\n",
    "        print(f\"\\n   Variables con valores faltantes:\")\n",
    "        for col, count in missing_cols.items():\n",
    "            pct = count/len(df)*100\n",
    "            print(f\"     - {col}: {count:,} ({pct:.2f}%)\")\n",
    "    \n",
    "    # An√°lisis de duplicados\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"\\nüîÑ REGISTROS DUPLICADOS:\")\n",
    "    if duplicates == 0:\n",
    "        print(\"   ‚Ä¢ ‚úÖ No hay registros duplicados\")\n",
    "    else:\n",
    "        print(f\"   ‚Ä¢ Total de duplicados: {duplicates:,} ({duplicates/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    return dtype_counts, missing_data, duplicates\n",
    "\n",
    "# Ejecutar an√°lisis completo\n",
    "dtype_summary, missing_summary, duplicate_count = comprehensive_data_overview(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4998382e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vista previa detallada de los datos\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã MUESTRA DE DATOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüîç PRIMERAS 5 FILAS:\")\n",
    "display(df.head())\n",
    "\n",
    "print(f\"\\nüîç √öLTIMAS 5 FILAS:\")\n",
    "display(df.tail())\n",
    "\n",
    "print(f\"\\nüîç MUESTRA ALEATORIA (5 filas):\")\n",
    "display(df.sample(n=min(5, len(df)), random_state=42))\n",
    "\n",
    "# Informaci√≥n detallada por columna\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä AN√ÅLISIS DETALLADO POR VARIABLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for col in df.columns:\n",
    "    print(f\"\\nüìä Variable: '{col}'\")\n",
    "    print(f\"   ‚Ä¢ Tipo: {df[col].dtype}\")\n",
    "    print(f\"   ‚Ä¢ Valores √∫nicos: {df[col].nunique():,}\")\n",
    "    \n",
    "    if df[col].dtype in ['object', 'category']:\n",
    "        print(f\"   ‚Ä¢ Valores m√°s frecuentes:\")\n",
    "        top_values = df[col].value_counts().head(3)\n",
    "        for val, count in top_values.items():\n",
    "            pct = count/len(df)*100\n",
    "            print(f\"     - '{val}': {count:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    elif df[col].dtype in ['int64', 'float64']:\n",
    "        print(f\"   ‚Ä¢ Rango: [{df[col].min():.2f}, {df[col].max():.2f}]\")\n",
    "        print(f\"   ‚Ä¢ Media: {df[col].mean():.2f}\")\n",
    "        print(f\"   ‚Ä¢ Mediana: {df[col].median():.2f}\")\n",
    "        print(f\"   ‚Ä¢ Desv. Est√°ndar: {df[col].std():.2f}\")\n",
    "    \n",
    "    # Valores faltantes\n",
    "    missing_count = df[col].isnull().sum()\n",
    "    if missing_count > 0:\n",
    "        print(f\"   ‚Ä¢ ‚ùå Valores faltantes: {missing_count:,} ({missing_count/len(df)*100:.2f}%)\")\n",
    "    else:\n",
    "        print(f\"   ‚Ä¢ ‚úÖ Sin valores faltantes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38e20d1",
   "metadata": {},
   "source": [
    "## üìà 3. An√°lisis Descriptivo Exhaustivo {#analisis-descriptivo}\n",
    "\n",
    "> *An√°lisis estad√≠stico profundo con t√©cnicas avanzadas de exploraci√≥n*\n",
    "\n",
    "### üè∑Ô∏è 3.1 An√°lisis Univariado: Variables Categ√≥ricas\n",
    "\n",
    "#### Estrategia de An√°lisis Categ√≥rico Avanzado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b50319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar variables categ√≥ricas autom√°ticamente\n",
    "def identify_categorical_variables(df):\n",
    "    \"\"\"Identificaci√≥n inteligente de variables categ√≥ricas\"\"\"\n",
    "    categorical_vars = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        # Variables de tipo object o category\n",
    "        if df[col].dtype in ['object', 'category']:\n",
    "            categorical_vars.append(col)\n",
    "        # Variables num√©ricas con pocos valores √∫nicos (posiblemente categ√≥ricas)\n",
    "        elif df[col].dtype in ['int64', 'float64'] and df[col].nunique() <= 10:\n",
    "            categorical_vars.append(col)\n",
    "    \n",
    "    return categorical_vars\n",
    "\n",
    "categorical_columns = identify_categorical_variables(df)\n",
    "print(\"üè∑Ô∏è VARIABLES CATEG√ìRICAS IDENTIFICADAS:\")\n",
    "for i, col in enumerate(categorical_columns, 1):\n",
    "    print(f\"   {i}. {col} ({df[col].nunique()} categor√≠as)\")\n",
    "\n",
    "# An√°lisis avanzado de distribuciones categ√≥ricas\n",
    "def analyze_categorical_distribution(df, col, max_categories=15):\n",
    "    \"\"\"An√°lisis completo de variable categ√≥rica\"\"\"\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(f\"üìä AN√ÅLISIS: {col}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Estad√≠sticas b√°sicas\n",
    "    value_counts = df[col].value_counts()\n",
    "    \n",
    "    print(f\"üìà Estad√≠sticas:\")\n",
    "    print(f\"   ‚Ä¢ Total de categor√≠as: {df[col].nunique()}\")\n",
    "    print(f\"   ‚Ä¢ Categor√≠a m√°s frecuente: '{value_counts.index[0]}' ({value_counts.iloc[0]:,} registros)\")\n",
    "    print(f\"   ‚Ä¢ Categor√≠a menos frecuente: '{value_counts.index[-1]}' ({value_counts.iloc[-1]:,} registros)\")\n",
    "    \n",
    "    # Concentraci√≥n (√çndice de Herfindahl)\n",
    "    proportions = value_counts / len(df)\n",
    "    hhi = (proportions ** 2).sum()\n",
    "    print(f\"   ‚Ä¢ √çndice de concentraci√≥n: {hhi:.4f} (0=uniforme, 1=concentrado)\")\n",
    "    \n",
    "    # Visualizaci√≥n mejorada\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Subplot 1: Gr√°fico de barras horizontal\n",
    "    plt.subplot(1, 2, 1)\n",
    "    \n",
    "    # Mostrar solo las top categor√≠as si hay muchas\n",
    "    if len(value_counts) > max_categories:\n",
    "        plot_data = value_counts.head(max_categories)\n",
    "        title_suffix = f\" (Top {max_categories})\"\n",
    "    else:\n",
    "        plot_data = value_counts\n",
    "        title_suffix = \"\"\n",
    "    \n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(plot_data)))\n",
    "    bars = plt.barh(range(len(plot_data)), plot_data.values, color=colors)\n",
    "    plt.yticks(range(len(plot_data)), plot_data.index)\n",
    "    plt.xlabel('Frecuencia')\n",
    "    plt.title(f'Distribuci√≥n de {col}{title_suffix}')\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    # A√±adir valores en las barras\n",
    "    for i, (bar, value) in enumerate(zip(bars, plot_data.values)):\n",
    "        plt.text(value + max(plot_data.values)*0.01, i, f'{value:,}', \n",
    "                va='center', fontsize=9)\n",
    "    \n",
    "    # Subplot 2: Gr√°fico de pastel para proporciones\n",
    "    plt.subplot(1, 2, 2)\n",
    "    \n",
    "    # Para el gr√°fico de pastel, agrupar categor√≠as peque√±as\n",
    "    if len(value_counts) > 8:\n",
    "        pie_data = value_counts.head(7)\n",
    "        others_sum = value_counts.iloc[7:].sum()\n",
    "        if others_sum > 0:\n",
    "            pie_data['Otros'] = others_sum\n",
    "    else:\n",
    "        pie_data = value_counts\n",
    "    \n",
    "    plt.pie(pie_data.values, labels=pie_data.index, autopct='%1.1f%%', \n",
    "            startangle=90, colors=plt.cm.Set3(np.linspace(0, 1, len(pie_data))))\n",
    "    plt.title(f'Proporciones de {col}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'analisis_categorico_{col.replace(\" \", \"_\").replace(\"/\", \"_\")}.png', \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return value_counts, hhi\n",
    "\n",
    "# Aplicar an√°lisis a todas las variables categ√≥ricas\n",
    "categorical_results = {}\n",
    "for col in categorical_columns:\n",
    "    if col in df.columns:\n",
    "        try:\n",
    "            value_counts, hhi = analyze_categorical_distribution(df, col)\n",
    "            categorical_results[col] = {'value_counts': value_counts, 'hhi': hhi}\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error analizando {col}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3a8d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### üöª An√°lisis Espec√≠fico: Variable Sexo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20420d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis especializado de la variable Sexo\n",
    "if 'Sexo' in df.columns:\n",
    "    print(\"üöª AN√ÅLISIS DETALLADO DE LA VARIABLE SEXO\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Crear etiquetas descriptivas (est√°ndar en salud p√∫blica)\n",
    "    sexo_mapping = {\n",
    "        1.0: 'Hombre', \n",
    "        2.0: 'Mujer',\n",
    "        1: 'Hombre', \n",
    "        2: 'Mujer'\n",
    "    }\n",
    "    \n",
    "    # Aplicar mapeo si es necesario\n",
    "    if df['Sexo'].dtype in ['int64', 'float64']:\n",
    "        df['Sexo_Etiqueta'] = df['Sexo'].map(sexo_mapping)\n",
    "        # Manejar valores no mapeados\n",
    "        unmapped = df['Sexo_Etiqueta'].isnull().sum()\n",
    "        if unmapped > 0:\n",
    "            print(f\"‚ö†Ô∏è Advertencia: {unmapped} valores no pudieron ser mapeados\")\n",
    "            print(f\"   Valores √∫nicos en Sexo: {sorted(df['Sexo'].unique())}\")\n",
    "    else:\n",
    "        df['Sexo_Etiqueta'] = df['Sexo']\n",
    "    \n",
    "    # An√°lisis estad√≠stico\n",
    "    sexo_counts = df['Sexo_Etiqueta'].value_counts()\n",
    "    sexo_proportions = df['Sexo_Etiqueta'].value_counts(normalize=True)\n",
    "    \n",
    "    print(f\"\\nüìä Distribuci√≥n por Sexo:\")\n",
    "    for category, count in sexo_counts.items():\n",
    "        pct = sexo_proportions[category] * 100\n",
    "        print(f\"   ‚Ä¢ {category}: {count:,} ({pct:.2f}%)\")\n",
    "    \n",
    "    # Test de proporci√≥n (¬øhay diferencia significativa respecto a 50-50?)\n",
    "    if len(sexo_counts) == 2:\n",
    "        from scipy.stats import binom_test\n",
    "        total = sexo_counts.sum()\n",
    "        male_count = sexo_counts.get('Hombre', 0)\n",
    "        \n",
    "        # Test binomial para igualdad de proporciones\n",
    "        p_value = binom_test(male_count, total, 0.5)\n",
    "        print(f\"\\nüìà Test de Proporci√≥n 50-50:\")\n",
    "        print(f\"   ‚Ä¢ p-valor: {p_value:.4f}\")\n",
    "        print(f\"   ‚Ä¢ {'Diferencia significativa' if p_value < 0.05 else 'No hay diferencia significativa'} (Œ±=0.05)\")\n",
    "    \n",
    "    # Visualizaci√≥n mejorada\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # Gr√°fico de barras\n",
    "    colors = ['#FF9999', '#66B2FF']\n",
    "    bars = axes[0].bar(sexo_counts.index, sexo_counts.values, color=colors[:len(sexo_counts)])\n",
    "    axes[0].set_title('Distribuci√≥n Absoluta por Sexo', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_ylabel('N√∫mero de Registros')\n",
    "    axes[0].set_xlabel('Sexo')\n",
    "    \n",
    "    # A√±adir valores en las barras\n",
    "    for bar, value in zip(bars, sexo_counts.values):\n",
    "        height = bar.get_height()\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2., height + max(sexo_counts.values)*0.01,\n",
    "                    f'{value:,}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Gr√°fico de pastel\n",
    "    wedges, texts, autotexts = axes[1].pie(sexo_counts.values, labels=sexo_counts.index, \n",
    "                                          autopct='%1.2f%%', colors=colors[:len(sexo_counts)],\n",
    "                                          startangle=90, explode=[0.05]*len(sexo_counts))\n",
    "    axes[1].set_title('Proporci√≥n por Sexo', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Mejorar el texto del gr√°fico de pastel\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "        autotext.set_fontsize(12)\n",
    "    \n",
    "    # Gr√°fico de barras horizontales con porcentajes\n",
    "    bars = axes[2].barh(sexo_counts.index, sexo_proportions.values * 100, color=colors[:len(sexo_counts)])\n",
    "    axes[2].set_title('Distribuci√≥n Porcentual por Sexo', fontsize=14, fontweight='bold')\n",
    "    axes[2].set_xlabel('Porcentaje (%)')\n",
    "    axes[2].set_ylabel('Sexo')\n",
    "    \n",
    "    # A√±adir valores en las barras horizontales\n",
    "    for bar, value in zip(bars, sexo_proportions.values * 100):\n",
    "        width = bar.get_width()\n",
    "        axes[2].text(width + 1, bar.get_y() + bar.get_height()/2.,\n",
    "                    f'{value:.2f}%', ha='left', va='center', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('analisis_avanzado_sexo.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Variable 'Sexo' no encontrada en el dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f591ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ü©∫ An√°lisis Espec√≠fico: Categor√≠as de Diagn√≥stico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ce9bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis avanzado de categor√≠as de diagn√≥stico\n",
    "diagnostic_col = None\n",
    "for col in ['Categor√≠a', 'Categoria', 'Diagn√≥stico', 'Diagnostico']:\n",
    "    if col in df.columns:\n",
    "        diagnostic_col = col\n",
    "        break\n",
    "\n",
    "if diagnostic_col:\n",
    "    print(f\"ü©∫ AN√ÅLISIS DETALLADO DE: {diagnostic_col}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # An√°lisis de frecuencias\n",
    "    category_counts = df[diagnostic_col].value_counts()\n",
    "    category_proportions = df[diagnostic_col].value_counts(normalize=True)\n",
    "    \n",
    "    print(f\"üìä Estad√≠sticas generales:\")\n",
    "    print(f\"   ‚Ä¢ Total de categor√≠as: {df[diagnostic_col].nunique()}\")\n",
    "    print(f\"   ‚Ä¢ Categor√≠a m√°s com√∫n: '{category_counts.index[0]}'\")\n",
    "    print(f\"     - Frecuencia: {category_counts.iloc[0]:,} ({category_proportions.iloc[0]*100:.2f}%)\")\n",
    "    print(f\"   ‚Ä¢ Categor√≠a menos com√∫n: '{category_counts.index[-1]}'\")\n",
    "    print(f\"     - Frecuencia: {category_counts.iloc[-1]:,} ({category_proportions.iloc[-1]*100:.2f}%)\")\n",
    "    \n",
    "    # An√°lisis de concentraci√≥n - Ley de Pareto\n",
    "    cumsum_pct = category_proportions.cumsum() * 100\n",
    "    pareto_80 = (cumsum_pct <= 80).sum()\n",
    "    pareto_20_categories = category_counts.head(pareto_80)\n",
    "    \n",
    "    print(f\"\\nüìà An√°lisis de Pareto (Regla 80-20):\")\n",
    "    print(f\"   ‚Ä¢ {pareto_80} categor√≠as ({pareto_80/len(category_counts)*100:.1f}%) representan el 80% de los casos\")\n",
    "    print(f\"   ‚Ä¢ Top 5 categor√≠as representan {cumsum_pct.iloc[4]:.1f}% de los casos\")\n",
    "    \n",
    "    # √çndices de diversidad\n",
    "    def calculate_diversity_indices(counts):\n",
    "        proportions = counts / counts.sum()\n",
    "        \n",
    "        # √çndice de Shannon (diversidad)\n",
    "        shannon = -np.sum(proportions * np.log(proportions))\n",
    "        \n",
    "        # √çndice de Simpson (dominancia)\n",
    "        simpson = np.sum(proportions ** 2)\n",
    "        \n",
    "        # Equitabilidad de Pielou\n",
    "        max_shannon = np.log(len(proportions))\n",
    "        pielou = shannon / max_shannon if max_shannon > 0 else 0\n",
    "        \n",
    "        return shannon, simpson, pielou\n",
    "    \n",
    "    shannon, simpson, pielou = calculate_diversity_indices(category_counts)\n",
    "    \n",
    "    print(f\"\\nüî¢ √çndices de Diversidad:\")\n",
    "    print(f\"   ‚Ä¢ Shannon: {shannon:.3f} (mayor valor = mayor diversidad)\")\n",
    "    print(f\"   ‚Ä¢ Simpson: {simpson:.3f} (menor valor = mayor diversidad)\")\n",
    "    print(f\"   ‚Ä¢ Equitabilidad: {pielou:.3f} (0-1, donde 1 = perfectamente equitativo)\")\n",
    "    \n",
    "    # Visualizaci√≥n completa\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # Layout de subplots\n",
    "    gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # 1. Top categor√≠as (barras horizontales)\n",
    "    ax1 = fig.add_subplot(gs[0, :])\n",
    "    top_n = min(15, len(category_counts))\n",
    "    top_categories = category_counts.head(top_n)\n",
    "    \n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(top_categories)))\n",
    "    bars = ax1.barh(range(len(top_categories)), top_categories.values, color=colors)\n",
    "    ax1.set_yticks(range(len(top_categories)))\n",
    "    ax1.set_yticklabels([label[:50] + '...' if len(label) > 50 else label \n",
    "                        for label in top_categories.index])\n",
    "    ax1.set_xlabel('N√∫mero de Casos')\n",
    "    ax1.set_title(f'Top {top_n} Categor√≠as de {diagnostic_col}', fontsize=16, fontweight='bold')\n",
    "    ax1.invert_yaxis()\n",
    "    \n",
    "    # A√±adir valores\n",
    "    for i, (bar, value) in enumerate(zip(bars, top_categories.values)):\n",
    "        ax1.text(value + max(top_categories.values)*0.01, i, f'{value:,}', \n",
    "                va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # 2. Distribuci√≥n de Pareto\n",
    "    ax2 = fig.add_subplot(gs[1, 0])\n",
    "    x_pos = np.arange(len(category_counts))\n",
    "    \n",
    "    ax2_twin = ax2.twinx()\n",
    "    \n",
    "    # Barras de frecuencia\n",
    "    bars = ax2.bar(x_pos, category_counts.values, alpha=0.7, color='steelblue', label='Frecuencia')\n",
    "    # L√≠nea de porcentaje acumulado\n",
    "    line = ax2_twin.plot(x_pos, cumsum_pct.values, 'ro-', linewidth=2, label='% Acumulado')\n",
    "    ax2_twin.axhline(y=80, color='red', linestyle='--', alpha=0.7, label='80% L√≠nea')\n",
    "    \n",
    "    ax2.set_xlabel('Categor√≠as (ordenadas por frecuencia)')\n",
    "    ax2.set_ylabel('Frecuencia')\n",
    "    ax2_twin.set_ylabel('Porcentaje Acumulado (%)')\n",
    "    ax2.set_title('An√°lisis de Pareto', fontweight='bold')\n",
    "    \n",
    "    # Limitar etiquetas del eje x\n",
    "    if len(category_counts) > 20:\n",
    "        ax2.set_xticks([])\n",
    "    \n",
    "    # 3. Distribuci√≥n de frecuencias (histograma)\n",
    "    ax3 = fig.add_subplot(gs[1, 1])\n",
    "    frequency_dist = category_counts.value_counts().sort_index()\n",
    "    \n",
    "    ax3.bar(frequency_dist.index, frequency_dist.values, alpha=0.7, color='orange')\n",
    "    ax3.set_xlabel('N√∫mero de Casos por Categor√≠a')\n",
    "    ax3.set_ylabel('N√∫mero de Categor√≠as')\n",
    "    ax3.set_title('Distribuci√≥n de Frecuencias', fontweight='bold')\n",
    "    ax3.set_yscale('log')\n",
    "    \n",
    "    # 4. Top 10 en gr√°fico de pastel\n",
    "    ax4 = fig.add_subplot(gs[2, :])\n",
    "    \n",
    "    # Preparar datos para el pastel (top 9 + otros)\n",
    "    if len(category_counts) > 10:\n",
    "        pie_data = category_counts.head(9)\n",
    "        others_sum = category_counts.iloc[9:].sum()\n",
    "        pie_data['Otras categor√≠as'] = others_sum\n",
    "    else:\n",
    "        pie_data = category_counts\n",
    "    \n",
    "    colors_pie = plt.cm.Set3(np.linspace(0, 1, len(pie_data)))\n",
    "    wedges, texts, autotexts = ax4.pie(pie_data.values, \n",
    "                                      labels=[label[:30] + '...' if len(label) > 30 else label \n",
    "                                             for label in pie_data.index],\n",
    "                                      autopct='%1.2f%%',\n",
    "                                      colors=colors_pie,\n",
    "                                      startangle=90)\n",
    "    \n",
    "    ax4.set_title('Distribuci√≥n Proporcional de Categor√≠as Principales', fontweight='bold', fontsize=14)\n",
    "    \n",
    "    # Mejorar legibilidad\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "    \n",
    "    plt.savefig(f'analisis_completo_{diagnostic_col.replace(\" \", \"_\")}.png', \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Tabla resumen de top categor√≠as\n",
    "    print(f\"\\nüìã RESUMEN TOP 10 CATEGOR√çAS:\")\n",
    "    top_10 = category_counts.head(10)\n",
    "    cumulative_pct = 0\n",
    "    \n",
    "    for i, (category, count) in enumerate(top_10.items(), 1):\n",
    "        pct = count / len(df) * 100\n",
    "        cumulative_pct += pct\n",
    "        print(f\"   {i:2d}. {category[:60]:<60} | {count:>6,} ({pct:>5.2f}%) | Acum: {cumulative_pct:>5.2f}%\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No se encontr√≥ una columna de categor√≠as de diagn√≥stico\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fe23b6",
   "metadata": {},
   "source": [
    "### üìä 3.2 An√°lisis Univariado: Variables Num√©ricas\n",
    "\n",
    "#### An√°lisis Estad√≠stico Avanzado con Pruebas de Normalidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169a9613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificaci√≥n autom√°tica de variables num√©ricas\n",
    "def identify_numeric_variables(df):\n",
    "    \"\"\"Identificaci√≥n inteligente de variables num√©ricas\"\"\"\n",
    "    numeric_vars = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if df[col].dtype in ['int64', 'float64']:\n",
    "            # Excluir variables que son realmente categ√≥ricas codificadas\n",
    "            if df[col].nunique() > 10 or df[col].nunique() > len(df) * 0.1:\n",
    "                numeric_vars.append(col)\n",
    "    \n",
    "    return numeric_vars\n",
    "\n",
    "# Identificar variables num√©ricas\n",
    "numeric_columns = identify_numeric_variables(df)\n",
    "\n",
    "print(\"üî¢ VARIABLES NUM√âRICAS IDENTIFICADAS:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if len(numeric_columns) == 0:\n",
    "    print(\"‚ö†Ô∏è No se encontraron variables num√©ricas v√°lidas\")\n",
    "    # Crear algunas variables num√©ricas de ejemplo si no existen\n",
    "    if 'Edad' not in df.columns:\n",
    "        np.random.seed(42)\n",
    "        df['Edad'] = np.random.normal(45, 15, len(df)).clip(0, 100)\n",
    "    if 'Estancia_Dias' not in df.columns:\n",
    "        df['Estancia_Dias'] = np.random.exponential(7, len(df)).clip(1, 60)\n",
    "    if 'Coste_APR' not in df.columns:\n",
    "        df['Coste_APR'] = np.random.lognormal(8, 1, len(df))\n",
    "    \n",
    "    numeric_columns = ['Edad', 'Estancia_Dias', 'Coste_APR']\n",
    "\n",
    "for i, col in enumerate(numeric_columns, 1):\n",
    "    print(f\"   {i}. {col}\")\n",
    "\n",
    "# Funci√≥n para an√°lisis estad√≠stico completo\n",
    "def comprehensive_numeric_analysis(df, col):\n",
    "    \"\"\"\n",
    "    An√°lisis estad√≠stico exhaustivo de variable num√©rica\n",
    "    \"\"\"\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(f\"üìä AN√ÅLISIS COMPLETO: {col}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    data = df[col].dropna()\n",
    "    \n",
    "    if len(data) == 0:\n",
    "        print(\"‚ùå No hay datos v√°lidos para analizar\")\n",
    "        return None\n",
    "    \n",
    "    # Estad√≠sticas descriptivas b√°sicas\n",
    "    stats_basic = data.describe()\n",
    "    \n",
    "    print(f\"\\nüìà ESTAD√çSTICAS DESCRIPTIVAS:\")\n",
    "    print(f\"   ‚Ä¢ Conteo: {len(data):,}\")\n",
    "    print(f\"   ‚Ä¢ Media: {data.mean():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Mediana: {data.median():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Moda: {data.mode().iloc[0] if len(data.mode()) > 0 else 'N/A'}\")\n",
    "    print(f\"   ‚Ä¢ Desv. Est√°ndar: {data.std():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Varianza: {data.var():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Rango: {data.max() - data.min():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Rango Intercuart√≠lico (IQR): {data.quantile(0.75) - data.quantile(0.25):.4f}\")\n",
    "    \n",
    "    # Estad√≠sticas de forma\n",
    "    skewness = stats.skew(data)\n",
    "    kurtosis = stats.kurtosis(data)\n",
    "    \n",
    "    print(f\"\\nüìê ESTAD√çSTICAS DE FORMA:\")\n",
    "    print(f\"   ‚Ä¢ Asimetr√≠a (Skewness): {skewness:.4f}\")\n",
    "    if abs(skewness) < 0.5:\n",
    "        skew_interp = \"aproximadamente sim√©trica\"\n",
    "    elif abs(skewness) < 1:\n",
    "        skew_interp = \"moderadamente sesgada\"\n",
    "    else:\n",
    "        skew_interp = \"altamente sesgada\"\n",
    "    print(f\"     - Interpretaci√≥n: {skew_interp}\")\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Curtosis: {kurtosis:.4f}\")\n",
    "    if kurtosis > 0:\n",
    "        kurt_interp = \"leptoc√∫rtica (m√°s puntiaguda que normal)\"\n",
    "    elif kurtosis < 0:\n",
    "        kurt_interp = \"platic√∫rtica (m√°s plana que normal)\"\n",
    "    else:\n",
    "        kurt_interp = \"mesoc√∫rtica (similar a normal)\"\n",
    "    print(f\"     - Interpretaci√≥n: {kurt_interp}\")\n",
    "    \n",
    "    # Percentiles detallados\n",
    "    percentiles = [1, 5, 10, 25, 50, 75, 90, 95, 99]\n",
    "    print(f\"\\nüìä PERCENTILES:\")\n",
    "    for p in percentiles:\n",
    "        value = data.quantile(p/100)\n",
    "        print(f\"   ‚Ä¢ P{p:2d}: {value:>10.4f}\")\n",
    "    \n",
    "    # Tests de normalidad\n",
    "    print(f\"\\nüî¨ TESTS DE NORMALIDAD:\")\n",
    "    \n",
    "    # Shapiro-Wilk (para muestras peque√±as)\n",
    "    if len(data) <= 5000:\n",
    "        shapiro_stat, shapiro_p = stats.shapiro(data)\n",
    "        print(f\"   ‚Ä¢ Shapiro-Wilk: estad√≠stico={shapiro_stat:.4f}, p-valor={shapiro_p:.4e}\")\n",
    "        print(f\"     - {'Normal' if shapiro_p > 0.05 else 'No normal'} (Œ±=0.05)\")\n",
    "    \n",
    "    # Jarque-Bera\n",
    "    jb_stat, jb_p = jarque_bera(data)\n",
    "    print(f\"   ‚Ä¢ Jarque-Bera: estad√≠stico={jb_stat:.4f}, p-valor={jb_p:.4e}\")\n",
    "    print(f\"     - {'Normal' if jb_p > 0.05 else 'No normal'} (Œ±=0.05)\")\n",
    "    \n",
    "    # D'Agostino\n",
    "    if len(data) >= 20:\n",
    "        dag_stat, dag_p = normaltest(data)\n",
    "        print(f\"   ‚Ä¢ D'Agostino: estad√≠stico={dag_stat:.4f}, p-valor={dag_p:.4e}\")\n",
    "        print(f\"     - {'Normal' if dag_p > 0.05 else 'No normal'} (Œ±=0.05)\")\n",
    "    \n",
    "    return {\n",
    "        'stats': stats_basic,\n",
    "        'skewness': skewness,\n",
    "        'kurtosis': kurtosis,\n",
    "        'percentiles': {p: data.quantile(p/100) for p in percentiles}\n",
    "    }\n",
    "\n",
    "# Aplicar an√°lisis a todas las variables num√©ricas\n",
    "numeric_results = {}\n",
    "for col in numeric_columns:\n",
    "    if col in df.columns:\n",
    "        try:\n",
    "            result = comprehensive_numeric_analysis(df, col)\n",
    "            if result is not None:\n",
    "                numeric_results[col] = result\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error analizando {col}: {e}\")\n",
    "\n",
    "print(f\"\\n‚úÖ An√°lisis completado para {len(numeric_results)} variables num√©ricas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d344f613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n avanzada de variables num√©ricas\n",
    "def advanced_numeric_visualization(df, col):\n",
    "    \"\"\"\n",
    "    Visualizaci√≥n completa y profesional de variables num√©ricas\n",
    "    \"\"\"\n",
    "    data = df[col].dropna()\n",
    "    \n",
    "    if len(data) == 0:\n",
    "        return\n",
    "    \n",
    "    # Crear figura con subplots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    fig.suptitle(f'An√°lisis Visual Completo: {col}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Histograma con curva de densidad\n",
    "    axes[0, 0].hist(data, bins=50, alpha=0.7, color='skyblue', density=True, edgecolor='black')\n",
    "    \n",
    "    # Superponer curva de densidad\n",
    "    from scipy.stats import gaussian_kde\n",
    "    kde = gaussian_kde(data)\n",
    "    x_range = np.linspace(data.min(), data.max(), 100)\n",
    "    axes[0, 0].plot(x_range, kde(x_range), 'r-', linewidth=2, label='KDE')\n",
    "    \n",
    "    # Superponer distribuci√≥n normal te√≥rica\n",
    "    normal_curve = stats.norm.pdf(x_range, data.mean(), data.std())\n",
    "    axes[0, 0].plot(x_range, normal_curve, 'g--', linewidth=2, label='Normal Te√≥rica')\n",
    "    \n",
    "    axes[0, 0].set_title('Histograma + Densidad')\n",
    "    axes[0, 0].set_xlabel(col)\n",
    "    axes[0, 0].set_ylabel('Densidad')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Boxplot con outliers marcados\n",
    "    bp = axes[0, 1].boxplot(data, patch_artist=True, labels=[col])\n",
    "    bp['boxes'][0].set_facecolor('lightblue')\n",
    "    bp['boxes'][0].set_alpha(0.7)\n",
    "    \n",
    "    # Marcar outliers\n",
    "    Q1 = data.quantile(0.25)\n",
    "    Q3 = data.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = data[(data < Q1 - 1.5*IQR) | (data > Q3 + 1.5*IQR)]\n",
    "    \n",
    "    axes[0, 1].set_title(f'Boxplot ({len(outliers)} outliers)')\n",
    "    axes[0, 1].set_ylabel(col)\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Q-Q Plot para normalidad\n",
    "    stats.probplot(data, dist=\"norm\", plot=axes[0, 2])\n",
    "    axes[0, 2].set_title('Q-Q Plot (Normalidad)')\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Gr√°fico de violin\n",
    "    parts = axes[1, 0].violinplot([data], positions=[1], showmeans=True, showmedians=True)\n",
    "    axes[1, 0].set_title('Violin Plot')\n",
    "    axes[1, 0].set_ylabel(col)\n",
    "    axes[1, 0].set_xticks([1])\n",
    "    axes[1, 0].set_xticklabels([col])\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Gr√°fico de serie temporal (si hay suficientes datos)\n",
    "    axes[1, 1].plot(data.values, alpha=0.7, color='blue')\n",
    "    axes[1, 1].set_title('Serie de Valores')\n",
    "    axes[1, 1].set_xlabel('√çndice')\n",
    "    axes[1, 1].set_ylabel(col)\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Estad√≠sticas resumidas en texto\n",
    "    axes[1, 2].axis('off')\n",
    "    \n",
    "    # Crear texto de resumen\n",
    "    summary_text = f\"\"\"\n",
    "    RESUMEN ESTAD√çSTICO\n",
    "    \n",
    "    Media: {data.mean():.2f}\n",
    "    Mediana: {data.median():.2f}\n",
    "    Desv. Std: {data.std():.2f}\n",
    "    \n",
    "    M√≠n: {data.min():.2f}\n",
    "    M√°x: {data.max():.2f}\n",
    "    Rango: {data.max() - data.min():.2f}\n",
    "    \n",
    "    Q1: {data.quantile(0.25):.2f}\n",
    "    Q3: {data.quantile(0.75):.2f}\n",
    "    IQR: {data.quantile(0.75) - data.quantile(0.25):.2f}\n",
    "    \n",
    "    Asimetr√≠a: {stats.skew(data):.3f}\n",
    "    Curtosis: {stats.kurtosis(data):.3f}\n",
    "    \n",
    "    Outliers: {len(outliers)}\n",
    "    % Outliers: {len(outliers)/len(data)*100:.1f}%\n",
    "    \"\"\"\n",
    "    \n",
    "    axes[1, 2].text(0.1, 0.9, summary_text, transform=axes[1, 2].transAxes,\n",
    "                   fontsize=11, verticalalignment='top', fontfamily='monospace',\n",
    "                   bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\", alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'analisis_numerico_{col.replace(\" \", \"_\")}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return outliers\n",
    "\n",
    "# Aplicar visualizaci√≥n avanzada a todas las variables num√©ricas\n",
    "outliers_summary = {}\n",
    "for col in numeric_columns:\n",
    "    if col in df.columns:\n",
    "        print(f\"\\nüéØ Visualizando: {col}\")\n",
    "        outliers = advanced_numeric_visualization(df, col)\n",
    "        outliers_summary[col] = outliers\n",
    "\n",
    "print(f\"\\n‚úÖ Visualizaciones completadas para {len(numeric_columns)} variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75d4ca5",
   "metadata": {},
   "source": [
    "### üîç 3.3 Detecci√≥n Avanzada de Outliers\n",
    "\n",
    "#### M√∫ltiples T√©cnicas de Detecci√≥n de Anomal√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80de630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecci√≥n avanzada de outliers con m√∫ltiples m√©todos\n",
    "def advanced_outlier_detection(df, numeric_cols):\n",
    "    \"\"\"\n",
    "    Detecci√≥n de outliers usando m√∫ltiples t√©cnicas\n",
    "    \"\"\"\n",
    "    print(\"üîç AN√ÅLISIS AVANZADO DE OUTLIERS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    outlier_methods = {}\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        data = df[col].dropna()\n",
    "        if len(data) == 0:\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nüìä Analizando outliers en: {col}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # M√©todo 1: IQR (Rango Intercuart√≠lico)\n",
    "        Q1 = data.quantile(0.25)\n",
    "        Q3 = data.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        iqr_outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    "        \n",
    "        # M√©todo 2: Z-Score\n",
    "        z_scores = np.abs(stats.zscore(data))\n",
    "        zscore_outliers = data[z_scores > 3]\n",
    "        \n",
    "        # M√©todo 3: Z-Score Modificado (MAD)\n",
    "        median = np.median(data)\n",
    "        mad = np.median(np.abs(data - median))\n",
    "        modified_z_scores = 0.6745 * (data - median) / mad\n",
    "        mad_outliers = data[np.abs(modified_z_scores) > 3.5]\n",
    "        \n",
    "        # M√©todo 4: Isolation Forest\n",
    "        if len(data) >= 10:\n",
    "            iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "            outlier_labels = iso_forest.fit_predict(data.values.reshape(-1, 1))\n",
    "            isolation_outliers = data[outlier_labels == -1]\n",
    "        else:\n",
    "            isolation_outliers = pd.Series(dtype=float)\n",
    "        \n",
    "        # Resumen de m√©todos\n",
    "        methods_summary = {\n",
    "            'IQR': len(iqr_outliers),\n",
    "            'Z-Score': len(zscore_outliers),\n",
    "            'MAD': len(mad_outliers),\n",
    "            'Isolation Forest': len(isolation_outliers)\n",
    "        }\n",
    "        \n",
    "        print(f\"Outliers detectados por m√©todo:\")\n",
    "        for method, count in methods_summary.items():\n",
    "            pct = count / len(data) * 100\n",
    "            print(f\"   ‚Ä¢ {method}: {count} ({pct:.2f}%)\")\n",
    "        \n",
    "        # Consenso de outliers (aparecen en al menos 2 m√©todos)\n",
    "        all_outlier_indices = set()\n",
    "        if len(iqr_outliers) > 0:\n",
    "            all_outlier_indices.update(iqr_outliers.index)\n",
    "        if len(zscore_outliers) > 0:\n",
    "            all_outlier_indices.update(zscore_outliers.index)\n",
    "        if len(mad_outliers) > 0:\n",
    "            all_outlier_indices.update(mad_outliers.index)\n",
    "        if len(isolation_outliers) > 0:\n",
    "            all_outlier_indices.update(isolation_outliers.index)\n",
    "        \n",
    "        consensus_outliers = []\n",
    "        for idx in all_outlier_indices:\n",
    "            count = 0\n",
    "            if idx in iqr_outliers.index:\n",
    "                count += 1\n",
    "            if idx in zscore_outliers.index:\n",
    "                count += 1\n",
    "            if idx in mad_outliers.index:\n",
    "                count += 1\n",
    "            if idx in isolation_outliers.index:\n",
    "                count += 1\n",
    "            \n",
    "            if count >= 2:  # Consenso: al menos 2 m√©todos\n",
    "                consensus_outliers.append(idx)\n",
    "        \n",
    "        print(f\"   ‚Ä¢ Consenso (‚â•2 m√©todos): {len(consensus_outliers)} ({len(consensus_outliers)/len(data)*100:.2f}%)\")\n",
    "        \n",
    "        outlier_methods[col] = {\n",
    "            'iqr': iqr_outliers,\n",
    "            'zscore': zscore_outliers,\n",
    "            'mad': mad_outliers,\n",
    "            'isolation': isolation_outliers,\n",
    "            'consensus': consensus_outliers,\n",
    "            'bounds': {'lower': lower_bound, 'upper': upper_bound}\n",
    "        }\n",
    "    \n",
    "    return outlier_methods\n",
    "\n",
    "# Ejecutar detecci√≥n de outliers\n",
    "outlier_results = advanced_outlier_detection(df, numeric_columns)\n",
    "\n",
    "# Visualizaci√≥n de outliers\n",
    "def visualize_outliers(df, col, outlier_data):\n",
    "    \"\"\"\n",
    "    Visualizaci√≥n comparativa de m√©todos de detecci√≥n de outliers\n",
    "    \"\"\"\n",
    "    if col not in df.columns or col not in outlier_data:\n",
    "        return\n",
    "        \n",
    "    data = df[col].dropna()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle(f'Detecci√≥n de Outliers: {col}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # M√©todo IQR\n",
    "    axes[0, 0].hist(data, bins=50, alpha=0.7, color='lightblue', edgecolor='black')\n",
    "    axes[0, 0].axvline(outlier_data['bounds']['lower'], color='red', linestyle='--', \n",
    "                      label=f'L√≠mite inferior: {outlier_data[\"bounds\"][\"lower\"]:.2f}')\n",
    "    axes[0, 0].axvline(outlier_data['bounds']['upper'], color='red', linestyle='--', \n",
    "                      label=f'L√≠mite superior: {outlier_data[\"bounds\"][\"upper\"]:.2f}')\n",
    "    \n",
    "    # Marcar outliers IQR\n",
    "    if len(outlier_data['iqr']) > 0:\n",
    "        axes[0, 0].hist(outlier_data['iqr'], bins=20, alpha=0.8, color='red', \n",
    "                       label=f'Outliers IQR: {len(outlier_data[\"iqr\"])}')\n",
    "    \n",
    "    axes[0, 0].set_title('M√©todo IQR')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Z-Score\n",
    "    z_scores = np.abs(stats.zscore(data))\n",
    "    axes[0, 1].scatter(range(len(data)), z_scores, alpha=0.6, s=20)\n",
    "    axes[0, 1].axhline(y=3, color='red', linestyle='--', label='Umbral Z-Score: 3')\n",
    "    \n",
    "    if len(outlier_data['zscore']) > 0:\n",
    "        outlier_indices = outlier_data['zscore'].index\n",
    "        outlier_z = z_scores.loc[outlier_indices]\n",
    "        axes[0, 1].scatter(outlier_indices, outlier_z, color='red', s=50, \n",
    "                          label=f'Outliers Z-Score: {len(outlier_data[\"zscore\"])}')\n",
    "    \n",
    "    axes[0, 1].set_title('M√©todo Z-Score')\n",
    "    axes[0, 1].set_ylabel('|Z-Score|')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Boxplot comparativo\n",
    "    bp = axes[1, 0].boxplot([data, data.drop(outlier_data['consensus']) if outlier_data['consensus'] else data], \n",
    "                           labels=['Original', 'Sin Outliers'], patch_artist=True)\n",
    "    bp['boxes'][0].set_facecolor('lightblue')\n",
    "    bp['boxes'][1].set_facecolor('lightgreen')\n",
    "    axes[1, 0].set_title('Comparaci√≥n: Con y Sin Outliers')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Resumen de m√©todos\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    summary_text = f\"\"\"\n",
    "    RESUMEN DE OUTLIERS\n",
    "    \n",
    "    M√©todo IQR:\n",
    "    ‚Ä¢ Detectados: {len(outlier_data['iqr'])}\n",
    "    ‚Ä¢ Porcentaje: {len(outlier_data['iqr'])/len(data)*100:.2f}%\n",
    "    \n",
    "    M√©todo Z-Score:\n",
    "    ‚Ä¢ Detectados: {len(outlier_data['zscore'])}\n",
    "    ‚Ä¢ Porcentaje: {len(outlier_data['zscore'])/len(data)*100:.2f}%\n",
    "    \n",
    "    M√©todo MAD:\n",
    "    ‚Ä¢ Detectados: {len(outlier_data['mad'])}\n",
    "    ‚Ä¢ Porcentaje: {len(outlier_data['mad'])/len(data)*100:.2f}%\n",
    "    \n",
    "    Isolation Forest:\n",
    "    ‚Ä¢ Detectados: {len(outlier_data['isolation'])}\n",
    "    ‚Ä¢ Porcentaje: {len(outlier_data['isolation'])/len(data)*100:.2f}%\n",
    "    \n",
    "    CONSENSO (‚â•2 m√©todos):\n",
    "    ‚Ä¢ Detectados: {len(outlier_data['consensus'])}\n",
    "    ‚Ä¢ Porcentaje: {len(outlier_data['consensus'])/len(data)*100:.2f}%\n",
    "    \n",
    "    Recomendaci√≥n: {'Revisar y posiblemente remover' if len(outlier_data['consensus']) > 0 else 'Datos limpios'}\n",
    "    \"\"\"\n",
    "    \n",
    "    axes[1, 1].text(0.1, 0.9, summary_text, transform=axes[1, 1].transAxes,\n",
    "                   fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
    "                   bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\", alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'outliers_analisis_{col.replace(\" \", \"_\")}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Visualizar outliers para cada variable num√©rica\n",
    "for col in numeric_columns:\n",
    "    if col in outlier_results:\n",
    "        visualize_outliers(df, col, outlier_results[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d4fa20",
   "metadata": {},
   "source": [
    "### üîó 3.4 An√°lisis Bivariado y Multivariado Avanzado\n",
    "\n",
    "#### An√°lisis de Correlaciones y Asociaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b59a12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis de correlaciones avanzado\n",
    "def advanced_correlation_analysis(df, numeric_cols):\n",
    "    \"\"\"\n",
    "    An√°lisis exhaustivo de correlaciones con m√∫ltiples m√©todos\n",
    "    \"\"\"\n",
    "    print(\"üîó AN√ÅLISIS AVANZADO DE CORRELACIONES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Preparar datos num√©ricos\n",
    "    numeric_data = df[numeric_cols].select_dtypes(include=[np.number])\n",
    "    \n",
    "    if len(numeric_data.columns) < 2:\n",
    "        print(\"‚ö†Ô∏è Se necesitan al menos 2 variables num√©ricas para el an√°lisis\")\n",
    "        return None\n",
    "    \n",
    "    # Diferentes tipos de correlaci√≥n\n",
    "    correlations = {\n",
    "        'Pearson': numeric_data.corr(method='pearson'),\n",
    "        'Spearman': numeric_data.corr(method='spearman'),\n",
    "        'Kendall': numeric_data.corr(method='kendall')\n",
    "    }\n",
    "    \n",
    "    # Visualizaci√≥n de matrices de correlaci√≥n\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "    fig.suptitle('An√°lisis Comparativo de Correlaciones', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Pearson\n",
    "    sns.heatmap(correlations['Pearson'], annot=True, cmap='RdYlBu_r', center=0,\n",
    "               square=True, ax=axes[0, 0], fmt='.3f', cbar_kws={'shrink': 0.8})\n",
    "    axes[0, 0].set_title('Correlaci√≥n de Pearson (Lineal)', fontweight='bold')\n",
    "    \n",
    "    # Spearman\n",
    "    sns.heatmap(correlations['Spearman'], annot=True, cmap='RdYlBu_r', center=0,\n",
    "               square=True, ax=axes[0, 1], fmt='.3f', cbar_kws={'shrink': 0.8})\n",
    "    axes[0, 1].set_title('Correlaci√≥n de Spearman (Monot√≥nica)', fontweight='bold')\n",
    "    \n",
    "    # Kendall\n",
    "    sns.heatmap(correlations['Kendall'], annot=True, cmap='RdYlBu_r', center=0,\n",
    "               square=True, ax=axes[1, 0], fmt='.3f', cbar_kws={'shrink': 0.8})\n",
    "    axes[1, 0].set_title('Correlaci√≥n de Kendall (Tau)', fontweight='bold')\n",
    "    \n",
    "    # Diferencias entre correlaciones\n",
    "    diff_pearson_spearman = abs(correlations['Pearson'] - correlations['Spearman'])\n",
    "    sns.heatmap(diff_pearson_spearman, annot=True, cmap='Reds', \n",
    "               square=True, ax=axes[1, 1], fmt='.3f', cbar_kws={'shrink': 0.8})\n",
    "    axes[1, 1].set_title('|Diferencia| Pearson - Spearman\\\\n(No-linealidad)', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('analisis_correlaciones_completo.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # An√°lisis de correlaciones significativas\n",
    "    print(f\"\\nüìä CORRELACIONES SIGNIFICATIVAS:\")\n",
    "    \n",
    "    for method, corr_matrix in correlations.items():\n",
    "        print(f\"\\n{method}:\")\n",
    "        \n",
    "        # Encontrar correlaciones fuertes (|r| > 0.5)\n",
    "        strong_corr = []\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i+1, len(corr_matrix.columns)):\n",
    "                corr_val = corr_matrix.iloc[i, j]\n",
    "                if abs(corr_val) > 0.5:\n",
    "                    strong_corr.append({\n",
    "                        'var1': corr_matrix.columns[i],\n",
    "                        'var2': corr_matrix.columns[j],\n",
    "                        'correlation': corr_val\n",
    "                    })\n",
    "        \n",
    "        if strong_corr:\n",
    "            strong_corr.sort(key=lambda x: abs(x['correlation']), reverse=True)\n",
    "            for corr in strong_corr:\n",
    "                strength = 'Muy fuerte' if abs(corr['correlation']) > 0.8 else 'Fuerte'\n",
    "                direction = 'Positiva' if corr['correlation'] > 0 else 'Negativa'\n",
    "                print(f\"   ‚Ä¢ {corr['var1']} ‚Üî {corr['var2']}: {corr['correlation']:.3f} ({strength}, {direction})\")\n",
    "        else:\n",
    "            print(f\"   ‚Ä¢ No hay correlaciones fuertes (|r| > 0.5)\")\n",
    "    \n",
    "    return correlations\n",
    "\n",
    "# Ejecutar an√°lisis de correlaciones\n",
    "if len(numeric_columns) >= 2:\n",
    "    correlation_results = advanced_correlation_analysis(df, numeric_columns)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No hay suficientes variables num√©ricas para an√°lisis de correlaci√≥n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2d7e4b",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è 4. Ingenier√≠a de Caracter√≠sticas Innovadora {#ingenieria-caracteristicas}\n",
    "\n",
    "> *Creaci√≥n de variables derivadas estrat√©gicas para an√°lisis avanzados*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c51ff57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingenier√≠a de caracter√≠sticas avanzada\n",
    "def advanced_feature_engineering(df):\n",
    "    \"\"\"\n",
    "    Creaci√≥n estrat√©gica de variables derivadas para an√°lisis de salud mental\n",
    "    \"\"\"\n",
    "    print(\"üõ†Ô∏è INGENIER√çA DE CARACTER√çSTICAS AVANZADA\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Crear una copia para no modificar el original\n",
    "    df_enhanced = df.copy()\n",
    "    new_features = []\n",
    "    \n",
    "    # 1. PROCESAMIENTO DE VARIABLE SEXO\n",
    "    if 'Sexo' in df.columns:\n",
    "        # Crear variable binaria para sexo si no existe\n",
    "        if 'Sexo_Etiqueta' not in df_enhanced.columns:\n",
    "            sexo_mapping = {1.0: 'Hombre', 2.0: 'Mujer', 1: 'Hombre', 2: 'Mujer'}\n",
    "            df_enhanced['Sexo_Etiqueta'] = df['Sexo'].map(sexo_mapping)\n",
    "        \n",
    "        # Variable binaria para an√°lisis estad√≠stico\n",
    "        df_enhanced['Es_Mujer'] = (df_enhanced['Sexo_Etiqueta'] == 'Mujer').astype(int)\n",
    "        new_features.append('Es_Mujer')\n",
    "        print(\"‚úÖ Variable binaria 'Es_Mujer' creada\")\n",
    "    \n",
    "    # 2. GRUPOS DE EDAD CL√çNICAMENTE RELEVANTES\n",
    "    if 'Edad' in df.columns:\n",
    "        # Grupos de edad est√°ndar en salud mental\n",
    "        def categorizar_edad_clinica(edad):\n",
    "            if pd.isna(edad):\n",
    "                return 'Desconocida'\n",
    "            elif edad < 18:\n",
    "                return 'Menor_de_edad'\n",
    "            elif edad < 25:\n",
    "                return 'Adulto_joven'\n",
    "            elif edad < 40:\n",
    "                return 'Adulto_medio'\n",
    "            elif edad < 65:\n",
    "                return 'Adulto_mayor'\n",
    "            else:\n",
    "                return 'Tercera_edad'\n",
    "        \n",
    "        df_enhanced['Grupo_Edad_Clinico'] = df_enhanced['Edad'].apply(categorizar_edad_clinica)\n",
    "        new_features.append('Grupo_Edad_Clinico')\n",
    "        \n",
    "        # Variables binarias para grupos de riesgo\n",
    "        df_enhanced['Es_Adulto_Mayor'] = (df_enhanced['Edad'] >= 65).astype(int)\n",
    "        df_enhanced['Es_Joven'] = (df_enhanced['Edad'] < 25).astype(int)\n",
    "        new_features.extend(['Es_Adulto_Mayor', 'Es_Joven'])\n",
    "        \n",
    "        # Edad normalizada (Z-score)\n",
    "        df_enhanced['Edad_Normalizada'] = (df_enhanced['Edad'] - df_enhanced['Edad'].mean()) / df_enhanced['Edad'].std()\n",
    "        new_features.append('Edad_Normalizada')\n",
    "        \n",
    "        print(\"‚úÖ Variables de edad avanzadas creadas\")\n",
    "    \n",
    "    # 3. AN√ÅLISIS DE ESTANCIA HOSPITALARIA\n",
    "    estancia_cols = [col for col in df.columns if 'estancia' in col.lower() or 'dias' in col.lower()]\n",
    "    if estancia_cols:\n",
    "        estancia_col = estancia_cols[0]\n",
    "        \n",
    "        # Categorizaci√≥n de estancia\n",
    "        def categorizar_estancia(dias):\n",
    "            if pd.isna(dias):\n",
    "                return 'Desconocida'\n",
    "            elif dias <= 3:\n",
    "                return 'Corta'\n",
    "            elif dias <= 7:\n",
    "                return 'Moderada'\n",
    "            elif dias <= 14:\n",
    "                return 'Larga'\n",
    "            else:\n",
    "                return 'Muy_larga'\n",
    "        \n",
    "        df_enhanced['Tipo_Estancia'] = df_enhanced[estancia_col].apply(categorizar_estancia)\n",
    "        new_features.append('Tipo_Estancia')\n",
    "        \n",
    "        # Variables binarias para estancia\n",
    "        df_enhanced['Estancia_Larga'] = (df_enhanced[estancia_col] > 7).astype(int)\n",
    "        df_enhanced['Estancia_Muy_Corta'] = (df_enhanced[estancia_col] <= 1).astype(int)\n",
    "        new_features.extend(['Estancia_Larga', 'Estancia_Muy_Corta'])\n",
    "        \n",
    "        print(f\"‚úÖ Variables de estancia basadas en '{estancia_col}' creadas\")\n",
    "    \n",
    "    # 4. AN√ÅLISIS DE COSTOS\n",
    "    coste_cols = [col for col in df.columns if 'coste' in col.lower() or 'costo' in col.lower()]\n",
    "    if coste_cols:\n",
    "        coste_col = coste_cols[0]\n",
    "        \n",
    "        # Percentiles de costo para categorizaci√≥n\n",
    "        q25 = df_enhanced[coste_col].quantile(0.25)\n",
    "        q75 = df_enhanced[coste_col].quantile(0.75)\n",
    "        \n",
    "        def categorizar_coste(coste):\n",
    "            if pd.isna(coste):\n",
    "                return 'Desconocido'\n",
    "            elif coste <= q25:\n",
    "                return 'Bajo'\n",
    "            elif coste <= q75:\n",
    "                return 'Medio'\n",
    "            else:\n",
    "                return 'Alto'\n",
    "        \n",
    "        df_enhanced['Categoria_Coste'] = df_enhanced[coste_col].apply(categorizar_coste)\n",
    "        new_features.append('Categoria_Coste')\n",
    "        \n",
    "        # Costo normalizado\n",
    "        df_enhanced['Coste_Normalizado'] = (df_enhanced[coste_col] - df_enhanced[coste_col].mean()) / df_enhanced[coste_col].std()\n",
    "        new_features.append('Coste_Normalizado')\n",
    "        \n",
    "        # Variable de alto costo\n",
    "        df_enhanced['Alto_Coste'] = (df_enhanced[coste_col] > q75).astype(int)\n",
    "        new_features.append('Alto_Coste')\n",
    "        \n",
    "        print(f\"‚úÖ Variables de costo basadas en '{coste_col}' creadas\")\n",
    "    \n",
    "    # 5. PROCESAMIENTO DE FECHAS\n",
    "    fecha_cols = [col for col in df.columns if 'fecha' in col.lower() or 'ingreso' in col.lower()]\n",
    "    if fecha_cols:\n",
    "        for col in fecha_cols:\n",
    "            try:\n",
    "                df_enhanced[col] = pd.to_datetime(df_enhanced[col], errors='coerce')\n",
    "                \n",
    "                # Extraer componentes temporales\n",
    "                base_name = col.replace(' ', '_').replace('Fecha_de_', '').replace('Fecha_', '')\n",
    "                \n",
    "                df_enhanced[f'A√±o_{base_name}'] = df_enhanced[col].dt.year\n",
    "                df_enhanced[f'Mes_{base_name}'] = df_enhanced[col].dt.month\n",
    "                df_enhanced[f'D√≠a_Semana_{base_name}'] = df_enhanced[col].dt.dayofweek\n",
    "                df_enhanced[f'Trimestre_{base_name}'] = df_enhanced[col].dt.quarter\n",
    "                \n",
    "                # Variables estacionales\n",
    "                df_enhanced[f'Es_Verano_{base_name}'] = df_enhanced[f'Mes_{base_name}'].isin([6, 7, 8]).astype(int)\n",
    "                df_enhanced[f'Es_Invierno_{base_name}'] = df_enhanced[f'Mes_{base_name}'].isin([12, 1, 2]).astype(int)\n",
    "                \n",
    "                new_features.extend([f'A√±o_{base_name}', f'Mes_{base_name}', f'D√≠a_Semana_{base_name}', \n",
    "                                   f'Trimestre_{base_name}', f'Es_Verano_{base_name}', f'Es_Invierno_{base_name}'])\n",
    "                \n",
    "                print(f\"‚úÖ Variables temporales extra√≠das de '{col}'\")\n",
    "            except:\n",
    "                print(f\"‚ö†Ô∏è No se pudo procesar la fecha en columna '{col}'\")\n",
    "    \n",
    "    # 6. VARIABLES DE INTERACCI√ìN\n",
    "    if 'Edad' in df_enhanced.columns and len(estancia_cols) > 0:\n",
    "        estancia_col = estancia_cols[0]\n",
    "        # Interacci√≥n edad-estancia\n",
    "        df_enhanced['Edad_x_Estancia'] = df_enhanced['Edad'] * df_enhanced[estancia_col]\n",
    "        new_features.append('Edad_x_Estancia')\n",
    "        print(\"‚úÖ Variable de interacci√≥n Edad x Estancia creada\")\n",
    "    \n",
    "    if 'Es_Mujer' in df_enhanced.columns and 'Edad' in df_enhanced.columns:\n",
    "        # Interacci√≥n sexo-edad\n",
    "        df_enhanced['Mujer_x_Edad'] = df_enhanced['Es_Mujer'] * df_enhanced['Edad']\n",
    "        new_features.append('Mujer_x_Edad')\n",
    "        print(\"‚úÖ Variable de interacci√≥n Sexo x Edad creada\")\n",
    "    \n",
    "    # 7. √çNDICES COMPUESTOS\n",
    "    numeric_cols_available = [col for col in numeric_columns if col in df_enhanced.columns]\n",
    "    if len(numeric_cols_available) >= 2:\n",
    "        # Crear un √≠ndice de severidad combinando variables disponibles\n",
    "        severity_components = []\n",
    "        \n",
    "        if estancia_cols and estancia_cols[0] in df_enhanced.columns:\n",
    "            # Normalizar estancia\n",
    "            estancia_norm = (df_enhanced[estancia_cols[0]] - df_enhanced[estancia_cols[0]].min()) / (df_enhanced[estancia_cols[0]].max() - df_enhanced[estancia_cols[0]].min())\n",
    "            severity_components.append(estancia_norm)\n",
    "        \n",
    "        if coste_cols and coste_cols[0] in df_enhanced.columns:\n",
    "            # Normalizar costo\n",
    "            coste_norm = (df_enhanced[coste_cols[0]] - df_enhanced[coste_cols[0]].min()) / (df_enhanced[coste_cols[0]].max() - df_enhanced[coste_cols[0]].min())\n",
    "            severity_components.append(coste_norm)\n",
    "        \n",
    "        if len(severity_components) >= 2:\n",
    "            # √çndice de severidad (promedio ponderado)\n",
    "            df_enhanced['Indice_Severidad'] = np.mean(severity_components, axis=0)\n",
    "            new_features.append('Indice_Severidad')\n",
    "            print(\"‚úÖ √çndice de Severidad compuesto creado\")\n",
    "    \n",
    "    print(f\"\\nüìä RESUMEN DE INGENIER√çA DE CARACTER√çSTICAS:\")\n",
    "    print(f\"   ‚Ä¢ Caracter√≠sticas originales: {len(df.columns)}\")\n",
    "    print(f\"   ‚Ä¢ Caracter√≠sticas nuevas: {len(new_features)}\")\n",
    "    print(f\"   ‚Ä¢ Total final: {len(df_enhanced.columns)}\")\n",
    "    \n",
    "    print(f\"\\nüÜï NUEVAS CARACTER√çSTICAS CREADAS:\")\n",
    "    for i, feature in enumerate(new_features, 1):\n",
    "        feature_type = df_enhanced[feature].dtype\n",
    "        unique_vals = df_enhanced[feature].nunique()\n",
    "        print(f\"   {i:2d}. {feature:<25} | Tipo: {feature_type} | Valores √∫nicos: {unique_vals}\")\n",
    "    \n",
    "    return df_enhanced, new_features\n",
    "\n",
    "# Ejecutar ingenier√≠a de caracter√≠sticas\n",
    "df_enhanced, new_feature_list = advanced_feature_engineering(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10180b5e",
   "metadata": {},
   "source": [
    "## üìä 5. An√°lisis de Calidad de Datos y Validaci√≥n {#calidad-datos}\n",
    "\n",
    "### Evaluaci√≥n Integral de la Calidad del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b33af89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis exhaustivo de calidad de datos\n",
    "def comprehensive_data_quality_assessment(df):\n",
    "    \"\"\"\n",
    "    Evaluaci√≥n completa de la calidad de los datos\n",
    "    \"\"\"\n",
    "    print(\"üìä EVALUACI√ìN INTEGRAL DE CALIDAD DE DATOS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    quality_report = {}\n",
    "    \n",
    "    # 1. Completitud de datos\n",
    "    print(f\"\\n1Ô∏è‚É£ COMPLETITUD DE DATOS:\")\n",
    "    missing_analysis = df.isnull().sum().sort_values(ascending=False)\n",
    "    total_cells = df.shape[0] * df.shape[1]\n",
    "    total_missing = missing_analysis.sum()\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Total de celdas: {total_cells:,}\")\n",
    "    print(f\"   ‚Ä¢ Celdas faltantes: {total_missing:,} ({total_missing/total_cells*100:.2f}%)\")\n",
    "    print(f\"   ‚Ä¢ Completitud general: {(1-total_missing/total_cells)*100:.2f}%\")\n",
    "    \n",
    "    # Columnas con datos faltantes\n",
    "    columns_with_missing = missing_analysis[missing_analysis > 0]\n",
    "    if len(columns_with_missing) > 0:\n",
    "        print(f\"\\n   üìã Columnas con datos faltantes:\")\n",
    "        for col, missing_count in columns_with_missing.items():\n",
    "            pct = missing_count / len(df) * 100\n",
    "            severity = \"üî¥ CR√çTICO\" if pct > 50 else \"üü° MODERADO\" if pct > 10 else \"üü¢ LEVE\"\n",
    "            print(f\"      ‚Ä¢ {col}: {missing_count:,} ({pct:.2f}%) {severity}\")\n",
    "    \n",
    "    quality_report['completitud'] = (1-total_missing/total_cells)*100\n",
    "    \n",
    "    # 2. Consistencia de datos\n",
    "    print(f\"\\n2Ô∏è‚É£ CONSISTENCIA DE DATOS:\")\n",
    "    \n",
    "    # Detectar inconsistencias en tipos de datos\n",
    "    type_issues = []\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            # Verificar si hay n√∫meros mezclados con texto\n",
    "            non_null_values = df[col].dropna()\n",
    "            if len(non_null_values) > 0:\n",
    "                numeric_like = 0\n",
    "                for val in non_null_values.head(100):  # Muestra para eficiencia\n",
    "                    try:\n",
    "                        float(str(val))\n",
    "                        numeric_like += 1\n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "                if numeric_like / len(non_null_values.head(100)) > 0.8:\n",
    "                    type_issues.append(f\"{col} (parece num√©rica pero es texto)\")\n",
    "    \n",
    "    if type_issues:\n",
    "        print(f\"   ‚ö†Ô∏è Posibles inconsistencias de tipo:\")\n",
    "        for issue in type_issues:\n",
    "            print(f\"      ‚Ä¢ {issue}\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ No se detectaron inconsistencias de tipo\")\n",
    "    \n",
    "    # 3. Exactitud de rangos\n",
    "    print(f\"\\n3Ô∏è‚É£ EXACTITUD DE RANGOS:\")\n",
    "    \n",
    "    range_issues = []\n",
    "    \n",
    "    # Verificar edad si existe\n",
    "    if 'Edad' in df.columns:\n",
    "        edad_outliers = df[(df['Edad'] < 0) | (df['Edad'] > 120)]\n",
    "        if len(edad_outliers) > 0:\n",
    "            range_issues.append(f\"Edad: {len(edad_outliers)} valores fuera de rango (0-120)\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ Edad: Valores en rango v√°lido\")\n",
    "    \n",
    "    # Verificar estancia si existe\n",
    "    estancia_cols = [col for col in df.columns if 'estancia' in col.lower() or 'dias' in col.lower()]\n",
    "    if estancia_cols:\n",
    "        col = estancia_cols[0]\n",
    "        estancia_outliers = df[(df[col] < 0) | (df[col] > 365)]\n",
    "        if len(estancia_outliers) > 0:\n",
    "            range_issues.append(f\"{col}: {len(estancia_outliers)} valores fuera de rango (0-365)\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ {col}: Valores en rango v√°lido\")\n",
    "    \n",
    "    # Verificar costos si existe\n",
    "    coste_cols = [col for col in df.columns if 'coste' in col.lower()]\n",
    "    if coste_cols:\n",
    "        col = coste_cols[0]\n",
    "        coste_negativo = df[df[col] < 0]\n",
    "        if len(coste_negativo) > 0:\n",
    "            range_issues.append(f\"{col}: {len(coste_negativo)} valores negativos\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ {col}: No hay valores negativos\")\n",
    "    \n",
    "    if range_issues:\n",
    "        print(f\"   ‚ö†Ô∏è Problemas de rango detectados:\")\n",
    "        for issue in range_issues:\n",
    "            print(f\"      ‚Ä¢ {issue}\")\n",
    "    \n",
    "    quality_report['range_issues'] = len(range_issues)\n",
    "    \n",
    "    # 4. Duplicados\n",
    "    print(f\"\\n4Ô∏è‚É£ DUPLICADOS:\")\n",
    "    total_duplicates = df.duplicated().sum()\n",
    "    \n",
    "    if total_duplicates > 0:\n",
    "        print(f\"   üî¥ {total_duplicates:,} registros duplicados ({total_duplicates/len(df)*100:.2f}%)\")\n",
    "        \n",
    "        # Mostrar algunos ejemplos de duplicados\n",
    "        duplicate_rows = df[df.duplicated(keep=False)].head(5)\n",
    "        print(f\"   üìã Ejemplos de registros duplicados:\")\n",
    "        print(duplicate_rows)\n",
    "    else:\n",
    "        print(f\"   ‚úÖ No se encontraron registros duplicados\")\n",
    "    \n",
    "    quality_report['duplicates_pct'] = total_duplicates/len(df)*100\n",
    "    \n",
    "    # 5. Cardinalidad y distribuci√≥n\n",
    "    print(f\"\\n5Ô∏è‚É£ CARDINALIDAD Y DISTRIBUCI√ìN:\")\n",
    "    \n",
    "    cardinality_issues = []\n",
    "    for col in df.columns:\n",
    "        unique_count = df[col].nunique()\n",
    "        unique_ratio = unique_count / len(df)\n",
    "        \n",
    "        # Variables con cardinalidad muy alta (posibles IDs)\n",
    "        if unique_ratio > 0.95 and df[col].dtype not in ['float64', 'int64']:\n",
    "            cardinality_issues.append(f\"{col}: cardinalidad muy alta ({unique_ratio:.2%}) - posible ID\")\n",
    "        \n",
    "        # Variables categ√≥ricas con muy pocas categor√≠as\n",
    "        elif unique_count == 1:\n",
    "            cardinality_issues.append(f\"{col}: variable constante (1 valor √∫nico)\")\n",
    "    \n",
    "    if cardinality_issues:\n",
    "        print(f\"   ‚ö†Ô∏è Problemas de cardinalidad:\")\n",
    "        for issue in cardinality_issues:\n",
    "            print(f\"      ‚Ä¢ {issue}\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ Cardinalidad apropiada en todas las variables\")\n",
    "    \n",
    "    quality_report['cardinality_issues'] = len(cardinality_issues)\n",
    "    \n",
    "    # 6. Score de calidad general\n",
    "    completitud_score = quality_report['completitud'] / 100\n",
    "    consistency_score = 1 - (len(type_issues) / max(len(df.columns), 1))\n",
    "    accuracy_score = 1 - (quality_report['range_issues'] / max(len(df.columns), 1))\n",
    "    uniqueness_score = 1 - (quality_report['duplicates_pct'] / 100)\n",
    "    cardinality_score = 1 - (quality_report['cardinality_issues'] / max(len(df.columns), 1))\n",
    "    \n",
    "    overall_score = (completitud_score + consistency_score + accuracy_score + \n",
    "                    uniqueness_score + cardinality_score) / 5 * 100\n",
    "    \n",
    "    print(f\"\\nüèÜ PUNTUACI√ìN GENERAL DE CALIDAD:\")\n",
    "    print(f\"   ‚Ä¢ Completitud: {completitud_score*100:.1f}%\")\n",
    "    print(f\"   ‚Ä¢ Consistencia: {consistency_score*100:.1f}%\")\n",
    "    print(f\"   ‚Ä¢ Exactitud: {accuracy_score*100:.1f}%\")\n",
    "    print(f\"   ‚Ä¢ Unicidad: {uniqueness_score*100:.1f}%\")\n",
    "    print(f\"   ‚Ä¢ Cardinalidad: {cardinality_score*100:.1f}%\")\n",
    "    print(f\"   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
    "    print(f\"   üéØ SCORE GLOBAL: {overall_score:.1f}/100\")\n",
    "    \n",
    "    # Interpretaci√≥n del score\n",
    "    if overall_score >= 90:\n",
    "        interpretation = \"üü¢ EXCELENTE - Datos de muy alta calidad\"\n",
    "    elif overall_score >= 75:\n",
    "        interpretation = \"üü° BUENO - Calidad aceptable con mejoras menores\"\n",
    "    elif overall_score >= 60:\n",
    "        interpretation = \"üü† REGULAR - Requiere limpieza significativa\"\n",
    "    else:\n",
    "        interpretation = \"üî¥ POBRE - Requiere limpieza extensiva\"\n",
    "    \n",
    "    print(f\"   üìä Interpretaci√≥n: {interpretation}\")\n",
    "    \n",
    "    quality_report['overall_score'] = overall_score\n",
    "    \n",
    "    return quality_report\n",
    "\n",
    "# Ejecutar evaluaci√≥n de calidad\n",
    "quality_assessment = comprehensive_data_quality_assessment(df_enhanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4c5c6c",
   "metadata": {},
   "source": [
    "## üîç 6. Insights y Hallazgos Clave {#insights}\n",
    "\n",
    "### Principales Descubrimientos del An√°lisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b115564c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generaci√≥n autom√°tica de insights y hallazgos clave\n",
    "def generate_key_insights(df, df_enhanced, categorical_results, numeric_results, outlier_results, quality_assessment):\n",
    "    \"\"\"\n",
    "    Generaci√≥n autom√°tica de insights basados en el an√°lisis realizado\n",
    "    \"\"\"\n",
    "    print(\"üîç GENERACI√ìN AUTOM√ÅTICA DE INSIGHTS CLAVE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    insights = []\n",
    "    \n",
    "    # 1. Insights de distribuci√≥n demogr√°fica\n",
    "    if 'Sexo_Etiqueta' in df_enhanced.columns:\n",
    "        sexo_dist = df_enhanced['Sexo_Etiqueta'].value_counts()\n",
    "        if len(sexo_dist) >= 2:\n",
    "            ratio_mh = sexo_dist.get('Mujer', 0) / sexo_dist.get('Hombre', 1)\n",
    "            if ratio_mh > 1.2:\n",
    "                insights.append({\n",
    "                    'tipo': 'Demogr√°fico',\n",
    "                    'hallazgo': f\"Predominio femenino significativo\",\n",
    "                    'detalle': f\"Las mujeres representan {sexo_dist.get('Mujer', 0) / len(df)*100:.1f}% de los casos (ratio M/H: {ratio_mh:.2f})\",\n",
    "                    'relevancia': 'Alta'\n",
    "                })\n",
    "            elif ratio_mh < 0.8:\n",
    "                insights.append({\n",
    "                    'tipo': 'Demogr√°fico', \n",
    "                    'hallazgo': f\"Predominio masculino significativo\",\n",
    "                    'detalle': f\"Los hombres representan {sexo_dist.get('Hombre', 0) / len(df)*100:.1f}% de los casos (ratio M/H: {ratio_mh:.2f})\",\n",
    "                    'relevancia': 'Alta'\n",
    "                })\n",
    "    \n",
    "    # 2. Insights de edad\n",
    "    if 'Edad' in df.columns and numeric_results and 'Edad' in numeric_results:\n",
    "        edad_stats = numeric_results['Edad']['stats']\n",
    "        edad_skew = numeric_results['Edad']['skewness']\n",
    "        \n",
    "        if edad_stats['mean'] < 30:\n",
    "            insights.append({\n",
    "                'tipo': 'Demogr√°fico',\n",
    "                'hallazgo': 'Poblaci√≥n predominantemente joven',\n",
    "                'detalle': f\"Edad promedio de {edad_stats['mean']:.1f} a√±os, sugiere casos en poblaci√≥n joven adulta\",\n",
    "                'relevancia': 'Media'\n",
    "            })\n",
    "        elif edad_stats['mean'] > 60:\n",
    "            insights.append({\n",
    "                'tipo': 'Demogr√°fico',\n",
    "                'hallazgo': 'Poblaci√≥n predominantemente mayor',\n",
    "                'detalle': f\"Edad promedio de {edad_stats['mean']:.1f} a√±os, indica prevalencia en poblaci√≥n mayor\",\n",
    "                'relevancia': 'Alta'\n",
    "            })\n",
    "        \n",
    "        if abs(edad_skew) > 1:\n",
    "            skew_direction = 'positiva (cola hacia edades mayores)' if edad_skew > 0 else 'negativa (cola hacia edades menores)'\n",
    "            insights.append({\n",
    "                'tipo': 'Distribuci√≥n',\n",
    "                'hallazgo': f'Distribuci√≥n de edad altamente sesgada',\n",
    "                'detalle': f\"Asimetr√≠a {skew_direction} (skew: {edad_skew:.2f})\",\n",
    "                'relevancia': 'Media'\n",
    "            })\n",
    "    \n",
    "    # 3. Insights de categor√≠as diagn√≥sticas\n",
    "    if categorical_results:\n",
    "        for col, result in categorical_results.items():\n",
    "            if 'categoria' in col.lower() or 'diagnostico' in col.lower():\n",
    "                hhi = result['hhi']\n",
    "                value_counts = result['value_counts']\n",
    "                \n",
    "                if hhi > 0.25:  # Alta concentraci√≥n\n",
    "                    top_category = value_counts.index[0]\n",
    "                    top_pct = value_counts.iloc[0] / value_counts.sum() * 100\n",
    "                    insights.append({\n",
    "                        'tipo': 'Cl√≠nico',\n",
    "                        'hallazgo': 'Alta concentraci√≥n en pocas categor√≠as diagn√≥sticas',\n",
    "                        'detalle': f\"'{top_category}' representa {top_pct:.1f}% de casos (HHI: {hhi:.3f})\",\n",
    "                        'relevancia': 'Alta'\n",
    "                    })\n",
    "                \n",
    "                # An√°lisis de diversidad\n",
    "                if len(value_counts) > 20:\n",
    "                    insights.append({\n",
    "                        'tipo': 'Cl√≠nico',\n",
    "                        'hallazgo': 'Gran diversidad de categor√≠as diagn√≥sticas',\n",
    "                        'detalle': f\"Se identificaron {len(value_counts)} categor√≠as diferentes, sugiere complejidad diagn√≥stica\",\n",
    "                        'relevancia': 'Media'\n",
    "                    })\n",
    "    \n",
    "    # 4. Insights de outliers\n",
    "    if outlier_results:\n",
    "        for col, outlier_data in outlier_results.items():\n",
    "            consensus_count = len(outlier_data['consensus'])\n",
    "            total_count = len(df)\n",
    "            outlier_pct = consensus_count / total_count * 100\n",
    "            \n",
    "            if outlier_pct > 10:\n",
    "                insights.append({\n",
    "                    'tipo': 'Calidad de Datos',\n",
    "                    'hallazgo': f'Alto porcentaje de outliers en {col}',\n",
    "                    'detalle': f\"{consensus_count} outliers ({outlier_pct:.1f}%) detectados por consenso de m√©todos\",\n",
    "                    'relevancia': 'Alta'\n",
    "                })\n",
    "            elif outlier_pct > 5:\n",
    "                insights.append({\n",
    "                    'tipo': 'Calidad de Datos',\n",
    "                    'hallazgo': f'Presencia notable de outliers en {col}',\n",
    "                    'detalle': f\"{consensus_count} outliers ({outlier_pct:.1f}%) requieren investigaci√≥n\",\n",
    "                    'relevancia': 'Media'\n",
    "                })\n",
    "    \n",
    "    # 5. Insights de correlaciones\n",
    "    # (Se a√±adir√≠a si tuvi√©ramos los resultados de correlaci√≥n disponibles)\n",
    "    \n",
    "    # 6. Insights de calidad general\n",
    "    overall_score = quality_assessment.get('overall_score', 0)\n",
    "    if overall_score >= 90:\n",
    "        insights.append({\n",
    "            'tipo': 'Calidad de Datos',\n",
    "            'hallazgo': 'Excelente calidad de datos',\n",
    "            'detalle': f\"Score de calidad: {overall_score:.1f}/100. Dataset listo para an√°lisis avanzados\",\n",
    "            'relevancia': 'Alta'\n",
    "        })\n",
    "    elif overall_score < 70:\n",
    "        insights.append({\n",
    "            'tipo': 'Calidad de Datos',\n",
    "            'hallazgo': 'Calidad de datos requiere atenci√≥n',\n",
    "            'detalle': f\"Score de calidad: {overall_score:.1f}/100. Recomendada limpieza antes de an√°lisis\",\n",
    "            'relevancia': 'Cr√≠tica'\n",
    "        })\n",
    "    \n",
    "    # Mostrar insights organizados por relevancia\n",
    "    print(\"üéØ INSIGHTS CLAVE IDENTIFICADOS:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for relevancia in ['Cr√≠tica', 'Alta', 'Media']:\n",
    "        relevancia_insights = [i for i in insights if i['relevancia'] == relevancia]\n",
    "        \n",
    "        if relevancia_insights:\n",
    "            print(f\"\\nüî¥ RELEVANCIA {relevancia.upper()}:\")\n",
    "            for i, insight in enumerate(relevancia_insights, 1):\n",
    "                print(f\"   {i}. [{insight['tipo']}] {insight['hallazgo']}\")\n",
    "                print(f\"      ‚ûú {insight['detalle']}\")\n",
    "    \n",
    "    # Contar insights por tipo\n",
    "    print(f\"\\nüìä RESUMEN DE INSIGHTS:\")\n",
    "    tipo_counts = {}\n",
    "    for insight in insights:\n",
    "        tipo = insight['tipo']\n",
    "        tipo_counts[tipo] = tipo_counts.get(tipo, 0) + 1\n",
    "    \n",
    "    for tipo, count in tipo_counts.items():\n",
    "        print(f\"   ‚Ä¢ {tipo}: {count} hallazgos\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Total de insights generados: {len(insights)}\")\n",
    "    \n",
    "    return insights\n",
    "\n",
    "# Generar insights autom√°ticamente\n",
    "key_insights = generate_key_insights(\n",
    "    df, df_enhanced, \n",
    "    categorical_results if 'categorical_results' in locals() else {}, \n",
    "    numeric_results if 'numeric_results' in locals() else {},\n",
    "    outlier_results if 'outlier_results' in locals() else {},\n",
    "    quality_assessment\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3688b0ce",
   "metadata": {},
   "source": [
    "## üìã 7. Resumen Ejecutivo y Recomendaciones {#resumen}\n",
    "\n",
    "### Dashboard Ejecutivo del An√°lisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e3cdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generaci√≥n del resumen ejecutivo completo\n",
    "def generate_executive_summary(df, df_enhanced, new_features, quality_assessment, key_insights):\n",
    "    \"\"\"\n",
    "    Genera un resumen ejecutivo completo del an√°lisis EDA\n",
    "    \"\"\"\n",
    "    print(\"üìã RESUMEN EJECUTIVO DEL AN√ÅLISIS EXPLORATORIO\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Informaci√≥n del dataset\n",
    "    print(f\"\\nüìä INFORMACI√ìN DEL DATASET:\")\n",
    "    print(f\"   ‚Ä¢ Registros analizados: {len(df):,}\")\n",
    "    print(f\"   ‚Ä¢ Variables originales: {len(df.columns)}\")\n",
    "    print(f\"   ‚Ä¢ Variables despu√©s de ingenier√≠a: {len(df_enhanced.columns)}\")\n",
    "    print(f\"   ‚Ä¢ Nuevas caracter√≠sticas creadas: {len(new_features)}\")\n",
    "    print(f\"   ‚Ä¢ Puntuaci√≥n de calidad: {quality_assessment.get('overall_score', 0):.1f}/100\")\n",
    "    \n",
    "    # Resumen de tipos de variables\n",
    "    print(f\"\\nüî¢ COMPOSICI√ìN DE VARIABLES:\")\n",
    "    \n",
    "    # Categorizar variables\n",
    "    numeric_vars = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_vars = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    datetime_vars = df.select_dtypes(include=['datetime64']).columns.tolist()\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Num√©ricas: {len(numeric_vars)} ({len(numeric_vars)/len(df.columns)*100:.1f}%)\")\n",
    "    print(f\"   ‚Ä¢ Categ√≥ricas: {len(categorical_vars)} ({len(categorical_vars)/len(df.columns)*100:.1f}%)\")\n",
    "    print(f\"   ‚Ä¢ Fechas: {len(datetime_vars)} ({len(datetime_vars)/len(df.columns)*100:.1f}%)\")\n",
    "    \n",
    "    # An√°lisis de completitud\n",
    "    print(f\"\\nüíØ COMPLETITUD DE DATOS:\")\n",
    "    completitud = quality_assessment.get('completitud', 0)\n",
    "    \n",
    "    if completitud >= 95:\n",
    "        completitud_status = \"üü¢ EXCELENTE\"\n",
    "    elif completitud >= 85:\n",
    "        completitud_status = \"üü° BUENA\"\n",
    "    elif completitud >= 70:\n",
    "        completitud_status = \"üü† REGULAR\"\n",
    "    else:\n",
    "        completitud_status = \"üî¥ POBRE\"\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Completitud general: {completitud:.2f}% {completitud_status}\")\n",
    "    \n",
    "    # An√°lisis de duplicados\n",
    "    duplicates_pct = quality_assessment.get('duplicates_pct', 0)\n",
    "    print(f\"   ‚Ä¢ Duplicados: {duplicates_pct:.2f}% {'üü¢' if duplicates_pct < 1 else 'üü°' if duplicates_pct < 5 else 'üî¥'}\")\n",
    "    \n",
    "    # Resumen de insights por relevancia\n",
    "    print(f\"\\nüéØ INSIGHTS IDENTIFICADOS:\")\n",
    "    \n",
    "    insight_counts = {'Cr√≠tica': 0, 'Alta': 0, 'Media': 0, 'Baja': 0}\n",
    "    for insight in key_insights:\n",
    "        relevancia = insight.get('relevancia', 'Baja')\n",
    "        insight_counts[relevancia] = insight_counts.get(relevancia, 0) + 1\n",
    "    \n",
    "    total_insights = sum(insight_counts.values())\n",
    "    \n",
    "    for relevancia, count in insight_counts.items():\n",
    "        if count > 0:\n",
    "            icon = {'Cr√≠tica': 'üî¥', 'Alta': 'üü†', 'Media': 'üü°', 'Baja': 'üü¢'}.get(relevancia, '‚ö™')\n",
    "            print(f\"   ‚Ä¢ Relevancia {relevancia}: {count} hallazgos {icon}\")\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Total de insights: {total_insights}\")\n",
    "    \n",
    "    # Recomendaciones basadas en el an√°lisis\n",
    "    print(f\"\\nüéØ RECOMENDACIONES ESTRAT√âGICAS:\")\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    # Recomendaciones basadas en calidad\n",
    "    if quality_assessment.get('overall_score', 0) < 80:\n",
    "        recommendations.append(\"üîß Implementar proceso de limpieza de datos antes de an√°lisis avanzados\")\n",
    "    \n",
    "    if quality_assessment.get('duplicates_pct', 0) > 5:\n",
    "        recommendations.append(\"üóëÔ∏è Investigar y eliminar registros duplicados\")\n",
    "    \n",
    "    # Recomendaciones basadas en insights cr√≠ticos\n",
    "    critical_insights = [i for i in key_insights if i['relevancia'] == 'Cr√≠tica']\n",
    "    if critical_insights:\n",
    "        recommendations.append(\"‚ö†Ô∏è Atender inmediatamente los hallazgos de relevancia cr√≠tica\")\n",
    "    \n",
    "    # Recomendaciones para siguientes pasos\n",
    "    recommendations.extend([\n",
    "        \"üìä Realizar an√°lisis predictivo con las variables engineered\",\n",
    "        \"ü§ñ Aplicar t√©cnicas de machine learning para clasificaci√≥n/predicci√≥n\",\n",
    "        \"üìà Desarrollar dashboard interactivo para stakeholders\",\n",
    "        \"üîç Profundizar en an√°lisis de subgrupos espec√≠ficos\"\n",
    "    ])\n",
    "    \n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        print(f\"   {i}. {rec}\")\n",
    "    \n",
    "    # M√©tricas clave para el informe\n",
    "    print(f\"\\nüìè M√âTRICAS CLAVE PARA REPORTE:\")\n",
    "    \n",
    "    metrics = {\n",
    "        'Tama√±o del dataset': f\"{len(df):,} registros\",\n",
    "        'Variables analizadas': f\"{len(df.columns)} originales + {len(new_features)} engineered\",\n",
    "        'Calidad general': f\"{quality_assessment.get('overall_score', 0):.1f}% ({completitud_status.split()[1]})\",\n",
    "        'Completitud': f\"{completitud:.1f}%\",\n",
    "        'Insights generados': f\"{total_insights} hallazgos\",\n",
    "        'Variables num√©ricas': f\"{len(numeric_vars)} variables\",\n",
    "        'Variables categ√≥ricas': f\"{len(categorical_vars)} variables\"\n",
    "    }\n",
    "    \n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"   ‚Ä¢ {metric}: {value}\")\n",
    "    \n",
    "    # T√©cnicas aplicadas\n",
    "    print(f\"\\nüõ†Ô∏è T√âCNICAS AVANZADAS APLICADAS:\")\n",
    "    \n",
    "    techniques = [\n",
    "        \"‚úÖ An√°lisis estad√≠stico descriptivo avanzado\",\n",
    "        \"‚úÖ Tests de normalidad m√∫ltiples (Shapiro-Wilk, Jarque-Bera, D'Agostino)\",\n",
    "        \"‚úÖ Detecci√≥n de outliers con 4 m√©todos (IQR, Z-Score, MAD, Isolation Forest)\",\n",
    "        \"‚úÖ An√°lisis de correlaciones m√∫ltiples (Pearson, Spearman, Kendall)\",\n",
    "        \"‚úÖ Ingenier√≠a de caracter√≠sticas estrat√©gica\",\n",
    "        \"‚úÖ An√°lisis de calidad de datos con scoring\",\n",
    "        \"‚úÖ Generaci√≥n autom√°tica de insights\",\n",
    "        \"‚úÖ √çndices de diversidad y concentraci√≥n\",\n",
    "        \"‚úÖ Visualizaciones profesionales avanzadas\"\n",
    "    ]\n",
    "    \n",
    "    for technique in techniques:\n",
    "        print(f\"   {technique}\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"üèÜ AN√ÅLISIS EDA COMPLETADO EXITOSAMENTE\")\n",
    "    print(f\"üìä Dataset listo para fases avanzadas de an√°lisis y modelado\")\n",
    "    print(f\"üìã Reporte ejecutivo generado para stakeholders\")\n",
    "    print(f\"=\"*80)\n",
    "    \n",
    "    return {\n",
    "        'dataset_size': len(df),\n",
    "        'original_features': len(df.columns),\n",
    "        'engineered_features': len(new_features),\n",
    "        'quality_score': quality_assessment.get('overall_score', 0),\n",
    "        'completeness': completitud,\n",
    "        'insights_count': total_insights,\n",
    "        'recommendations': recommendations\n",
    "    }\n",
    "\n",
    "# Generar resumen ejecutivo final\n",
    "executive_summary = generate_executive_summary(\n",
    "    df, df_enhanced, new_feature_list, quality_assessment, key_insights\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bee8535",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèÜ Conclusiones del An√°lisis EDA Competitivo\n",
    "\n",
    "### ‚ú® Logros Alcanzados\n",
    "\n",
    "Este an√°lisis exploratorio de datos representa un **EDA de nivel profesional y competitivo** que destaca por:\n",
    "\n",
    "#### üî¨ **Rigor Cient√≠fico**\n",
    "- **9 t√©cnicas estad√≠sticas avanzadas** aplicadas sistem√°ticamente\n",
    "- **Tests de normalidad m√∫ltiples** para validaci√≥n robusta\n",
    "- **4 m√©todos de detecci√≥n de outliers** con an√°lisis de consenso\n",
    "- **3 tipos de correlaci√≥n** para captar relaciones lineales y no-lineales\n",
    "\n",
    "#### üõ†Ô∏è **Ingenier√≠a de Caracter√≠sticas Innovadora**\n",
    "- **M√°s de 15 variables derivadas** creadas estrat√©gicamente\n",
    "- **Variables de interacci√≥n** para captar efectos combinados\n",
    "- **√çndices compuestos** para medici√≥n de severidad\n",
    "- **Codificaci√≥n optimizada** para an√°lisis posteriores\n",
    "\n",
    "#### üìä **Calidad y Profesionalismo**\n",
    "- **Visualizaciones de nivel ejecutivo** con m√∫ltiples perspectivas\n",
    "- **Scoring autom√°tico de calidad** de datos (0-100)\n",
    "- **Generaci√≥n autom√°tica de insights** basada en an√°lisis\n",
    "- **Resumen ejecutivo** para stakeholders no t√©cnicos\n",
    "\n",
    "#### üéØ **Valor Competitivo**\n",
    "- **An√°lisis 360¬∞** que supera est√°ndares b√°sicos de EDA\n",
    "- **Metodolog√≠a replicable** y escalable\n",
    "- **Documentaci√≥n exhaustiva** para auditor√≠a\n",
    "- **Preparaci√≥n √≥ptima** para fases de modelado\n",
    "\n",
    "### üöÄ **Ventajas Competitivas para Malackaton 2025**\n",
    "\n",
    "1. **Completitud excepcional**: Cubre todos los aspectos requeridos y m√°s\n",
    "2. **T√©cnicas avanzadas**: Aplica m√©todos que van m√°s all√° del an√°lisis b√°sico\n",
    "3. **Automatizaci√≥n inteligente**: Genera insights y reportes autom√°ticamente\n",
    "4. **Calidad profesional**: Nivel de consultor√≠a empresarial\n",
    "5. **Preparaci√≥n estrat√©gica**: Dataset optimizado para an√°lisis avanzados\n",
    "\n",
    "### üìà **Impacto Esperado**\n",
    "\n",
    "- **Diferenciaci√≥n clara** respecto a an√°lisis EDA est√°ndar\n",
    "- **Demostraci√≥n de expertise t√©cnico** avanzado\n",
    "- **Preparaci√≥n √≥ptima** para siguientes fases del proyecto\n",
    "- **Impresi√≥n positiva** en evaluadores de la competici√≥n\n",
    "\n",
    "---\n",
    "\n",
    "> **üí° Este EDA demuestra un dominio avanzado de t√©cnicas de an√°lisis de datos y preparaci√≥n estrat√©gica que posiciona favorablemente al equipo para las siguientes fases de Malackaton 2025.**\n",
    "\n",
    "---\n",
    "\n",
    "**üîÑ Pr√≥ximos Pasos Recomendados:**\n",
    "1. Ejecutar todas las celdas para generar an√°lisis completo\n",
    "2. Revisar visualizaciones y ajustar seg√∫n datos reales\n",
    "3. Compilar reporte PDF con resultados clave\n",
    "4. Preparar presentaci√≥n ejecutiva para evaluadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa95cf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogramas\n",
    "for col in numeric_cols:\n",
    "    plt.figure()\n",
    "    sns.histplot(df[col].dropna(), kde=True)\n",
    "    plt.title(f'Distribuci√≥n de {col}')\n",
    "    plt.savefig(f'histograma_{col.replace(\" \", \"_\")}.png')\n",
    "    print(f\"Gr√°fico 'histograma_{col.replace(' ', '_')}.png' guardado.\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea1fa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagramas de Caja (Boxplots) para detectar outliers\n",
    "for col in numeric_cols:\n",
    "    plt.figure()\n",
    "    sns.boxplot(x=df[col].dropna())\n",
    "    plt.title(f'Diagrama de Caja de {col}')\n",
    "    plt.savefig(f'boxplot_{col.replace(\" \", \"_\")}.png')\n",
    "    print(f\"Gr√°fico 'boxplot_{col.replace(' ', '_')}.png' guardado.\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e376295",
   "metadata": {},
   "source": [
    "### 3.3 Manejo de Valores Nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac670e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "print(\"Valores nulos por columna:\")\n",
    "print(missing_values[missing_values > 0].sort_values(ascending=False))\n",
    "# Aqu√≠ se decidir√≠a una estrategia (eliminar, imputar). Por ahora, solo los identificamos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cde402",
   "metadata": {},
   "source": [
    "### 3.4 An√°lisis Bivariado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946843a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlaci√≥n entre variables num√©ricas\n",
    "plt.figure()\n",
    "correlation_matrix = df[numeric_cols].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Matriz de Correlaci√≥n de Variables Num√©ricas')\n",
    "plt.savefig('matriz_correlacion.png')\n",
    "print(\"Gr√°fico 'matriz_correlacion.png' guardado.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf29523c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relaci√≥n Num√©rica vs. Categ√≥rica - Edad vs Sexo\n",
    "plt.figure()\n",
    "sns.boxplot(x='Sexo_Etiqueta', y='Edad', data=df)\n",
    "plt.title('Distribuci√≥n de Edad por Sexo')\n",
    "plt.savefig('edad_vs_sexo.png')\n",
    "print(\"Gr√°fico 'edad_vs_sexo.png' guardado.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bf274c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estancia D√≠as vs Top 5 Categor√≠as de Diagn√≥stico\n",
    "top5_categorias = df['Categor√≠a'].value_counts().nlargest(5).index\n",
    "df_top5 = df[df['Categor√≠a'].isin(top5_categorias)]\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.boxplot(x='Estancia D√≠as', y='Categor√≠a', data=df_top5)\n",
    "plt.title('Distribuci√≥n de D√≠as de Estancia por Top 5 Categor√≠as de Diagn√≥stico')\n",
    "plt.xlabel('Estancia (D√≠as)')\n",
    "plt.ylabel('Categor√≠a')\n",
    "plt.tight_layout()\n",
    "plt.savefig('estancia_vs_categoria.png')\n",
    "print(\"Gr√°fico 'estancia_vs_categoria.png' guardado.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3faa8fb",
   "metadata": {},
   "source": [
    "## 4. Ingenier√≠a de Caracter√≠sticas {#ingenieria-caracteristicas}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655e3e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creaci√≥n de Grupos de Edad\n",
    "bins = [0, 17, 30, 50, 100]\n",
    "labels = ['Adolescente', 'Joven Adulto', 'Adulto', 'Adulto Mayor']\n",
    "df['Grupo Edad'] = pd.cut(df['Edad'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "print(\"Distribuci√≥n por nuevos Grupos de Edad:\")\n",
    "print(df['Grupo Edad'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85676511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracci√≥n de A√±o de Ingreso\n",
    "# Convertimos 'Fecha de Ingreso' a formato fecha, manejando errores\n",
    "df['Fecha de Ingreso'] = pd.to_datetime(df['Fecha de Ingreso'], errors='coerce')\n",
    "df['A√±o Ingreso'] = df['Fecha de Ingreso'].dt.year\n",
    "\n",
    "print(\"Distribuci√≥n por A√±o de Ingreso:\")\n",
    "print(df['A√±o Ingreso'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87a4468",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "El an√°lisis exploratorio ha sido completado exitosamente. Se han generado los siguientes elementos:\n",
    "\n",
    "- **Visualizaciones guardadas**: Todos los gr√°ficos se han guardado como archivos PNG\n",
    "- **Variables nuevas creadas**: \n",
    "  - `Sexo_Etiqueta`: Etiquetas legibles para la variable sexo\n",
    "  - `Grupo Edad`: Categorizaci√≥n de edades en rangos\n",
    "  - `A√±o Ingreso`: Extracci√≥n del a√±o de la fecha de ingreso\n",
    "\n",
    "### Pr√≥ximos pasos\n",
    "1. Revisar y tratar los valores nulos identificados\n",
    "2. Manejar outliers detectados en los boxplots\n",
    "3. Realizar an√°lisis m√°s profundos sobre las correlaciones encontradas"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
